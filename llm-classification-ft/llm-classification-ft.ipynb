{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"18793371","cell_type":"markdown","source":"# LLM Classification Finetuning\n\nCompetition: https://www.kaggle.com/competitions/llm-classification-finetuning/overview\n\n## Submission File\n\nFor each ID in the test set, you must predict the probability for each target class. The file should contain a header and have the following format:\n\n```csv\nid,winner_model_a,winner_model_b,winner_tie\n136060,0.33,0,33,0.33\n211333,0.33,0,33,0.33\n1233961,0.33,0,33,0.33\netc\n```\n\nSubmission file must be named `submission.csv` in the `/kaggle/working/` directory.\n\n## Inputs\n\nInput files are in `/kaggle/input/llm-classification-finetuning/` directory if\nrunning on Kaggle.\n\n```\n/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n```","metadata":{}},{"id":"115dc832","cell_type":"code","source":"# Install required packages\n%pip install torch pandas tabulate transformers evaluate peft wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:37:53.620359Z","iopub.execute_input":"2025-06-19T21:37:53.620546Z","iopub.status.idle":"2025-06-19T21:39:14.452393Z","shell.execute_reply.started":"2025-06-19T21:37:53.620530Z","shell.execute_reply":"2025-06-19T21:39:14.451425Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec (from torch)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"76c343d7","cell_type":"code","source":"import os\nkaggle_run_type = os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\nprint(f\"KAGGLE_KERNEL_RUN_TYPE: {kaggle_run_type}\")\n\nON_KAGGLE = kaggle_run_type is not None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:39:14.454074Z","iopub.execute_input":"2025-06-19T21:39:14.454325Z","iopub.status.idle":"2025-06-19T21:39:14.458970Z","shell.execute_reply.started":"2025-06-19T21:39:14.454300Z","shell.execute_reply":"2025-06-19T21:39:14.458213Z"}},"outputs":[{"name":"stdout","text":"KAGGLE_KERNEL_RUN_TYPE: Interactive\n","output_type":"stream"}],"execution_count":2},{"id":"012b4c69","cell_type":"code","source":"# Setup wandb\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\nos.environ[\"WANDB_PROJECT\"] = \"llm-classification-ft-peft\"\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n\nif ON_KAGGLE:\n    os.environ[\"WANDB_HOST\"] = \"kaggle\"\n    \n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n    wandb.login(key=wandb_key)\nelse:\n    wandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:39:14.459643Z","iopub.execute_input":"2025-06-19T21:39:14.459937Z","iopub.status.idle":"2025-06-19T21:39:27.618149Z","shell.execute_reply.started":"2025-06-19T21:39:14.459913Z","shell.execute_reply":"2025-06-19T21:39:27.617605Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrklee3\u001b[0m (\u001b[33mdrklee3-kava-labs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":3},{"id":"f0893984","cell_type":"code","source":"BASE_PATH = '/kaggle/input/llm-classification-finetuning' if ON_KAGGLE else './data/'\n\nprint(f\"Using base path: {BASE_PATH}\")\n\nprint(\"Available files in base path:\")\nfor root, dirs, files in os.walk(BASE_PATH):\n    for file in files:\n        print(f\" - {os.path.join(root, file)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:39:27.619908Z","iopub.execute_input":"2025-06-19T21:39:27.620205Z","iopub.status.idle":"2025-06-19T21:39:27.625662Z","shell.execute_reply.started":"2025-06-19T21:39:27.620189Z","shell.execute_reply":"2025-06-19T21:39:27.625099Z"}},"outputs":[{"name":"stdout","text":"Using base path: /kaggle/input/llm-classification-finetuning\nAvailable files in base path:\n - /kaggle/input/llm-classification-finetuning/sample_submission.csv\n - /kaggle/input/llm-classification-finetuning/train.csv\n - /kaggle/input/llm-classification-finetuning/test.csv\n","output_type":"stream"}],"execution_count":4},{"id":"5acea90a","cell_type":"markdown","source":"# Data Inputs\n\nLet's load and look at what we got first for inputs.","metadata":{}},{"id":"e3410a4a","cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))\n\nsample_submission_df = pd.read_csv(os.path.join(BASE_PATH, 'sample_submission.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:39:27.626339Z","iopub.execute_input":"2025-06-19T21:39:27.626534Z","iopub.status.idle":"2025-06-19T21:39:31.061476Z","shell.execute_reply.started":"2025-06-19T21:39:27.626519Z","shell.execute_reply":"2025-06-19T21:39:31.060922Z"}},"outputs":[],"execution_count":5},{"id":"b4e59919","cell_type":"code","source":"print(f\"Train DataFrame shape: {train_df.shape}\")\nprint(f\"Test DataFrame shape: {test_df.shape}\")\nprint(f\"Sample Submission DataFrame shape: {sample_submission_df.shape}\")\n\nprint (\"-------------------------\")\n# Print types of each column\nprint(\"\\nColumn types in Train DataFrame:\")\nprint(train_df.dtypes)\n\nprint(\"\\nColumn types in Test DataFrame:\")\nprint(test_df.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:39:31.062235Z","iopub.execute_input":"2025-06-19T21:39:31.062447Z","iopub.status.idle":"2025-06-19T21:39:31.068664Z","shell.execute_reply.started":"2025-06-19T21:39:31.062430Z","shell.execute_reply":"2025-06-19T21:39:31.067873Z"}},"outputs":[{"name":"stdout","text":"Train DataFrame shape: (57477, 9)\nTest DataFrame shape: (3, 4)\nSample Submission DataFrame shape: (3, 4)\n-------------------------\n\nColumn types in Train DataFrame:\nid                 int64\nmodel_a           object\nmodel_b           object\nprompt            object\nresponse_a        object\nresponse_b        object\nwinner_model_a     int64\nwinner_model_b     int64\nwinner_tie         int64\ndtype: object\n\nColumn types in Test DataFrame:\nid             int64\nprompt        object\nresponse_a    object\nresponse_b    object\ndtype: object\n","output_type":"stream"}],"execution_count":6},{"id":"7c30020a","cell_type":"code","source":"print(\"First rows of each DataFrame:\")\n\nprint(\"\\nTrain DataFrame:\")\nprint(train_df.head(1).to_markdown())\n\nprint(\"\\nTest DataFrame:\")\nprint(test_df.head(1).to_markdown())\n\nprint(\"\\nSample Submission DataFrame:\")\nprint(sample_submission_df.head(1).to_markdown())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:39:31.069468Z","iopub.execute_input":"2025-06-19T21:39:31.069815Z","iopub.status.idle":"2025-06-19T21:39:31.106104Z","shell.execute_reply.started":"2025-06-19T21:39:31.069790Z","shell.execute_reply":"2025-06-19T21:39:31.105569Z"}},"outputs":[{"name":"stdout","text":"First rows of each DataFrame:\n\nTrain DataFrame:\n|    |    id | model_a            | model_b    | prompt                                                                                                                                                                | response_a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | response_b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |   winner_model_a |   winner_model_b |   winner_tie |\n|---:|------:|:-------------------|:-----------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------:|-----------------:|-------------:|\n|  0 | 30192 | gpt-4-1106-preview | gpt-4-0613 | [\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"] | [\"The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Arguments in favor:**\\n\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\n\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\n\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\n\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\n\\n**Arguments against:**\\n\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\n\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\n\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \\\"tokens,\\\" undermining their legitimacy and potentially leading to resentment among colleagues.\\n\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\n\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one's ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \\\"color-blind\\\" or \\\"gender-blind\\\" approach to hiring and promotions.\\n\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.\",\"Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\\n\\nLet's slice into the debate with a zest of fun:\\n\\n**Team Pineapple:** \\\"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It's the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\\\"\\n\\n**Team No-Pineapple:** \\\"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple's sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\\\"\\n\\nAt the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it's a match made in heaven, others say it's a culinary crime. But let's face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there's room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon app\\u00e9tit, or as they say in pineapple paradise, \\\"Aloha-ppetite!\\\" \\ud83c\\udf4d\\ud83c\\udf55\"] | [\"As an AI, I don't have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It's a topic with valid arguments on both sides, and it's up to each individual or organization to decide what they believe is right.\",\"As an AI, I don't eat, so I don't have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It's a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!\"] |                1 |                0 |            0 |\n\nTest DataFrame:\n|    |     id | prompt                                                                                 | response_a                      | response_b                                                                                                         |\n|---:|-------:|:---------------------------------------------------------------------------------------|:--------------------------------|:-------------------------------------------------------------------------------------------------------------------|\n|  0 | 136060 | [\"I have three oranges today, I ate an orange yesterday. How many oranges do I have?\"] | [\"You have two oranges today.\"] | [\"You still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\"] |\n\nSample Submission DataFrame:\n|    |     id |   winner_model_a |   winner_model_b |   winner_tie |\n|---:|-------:|-----------------:|-----------------:|-------------:|\n|  0 | 136060 |         0.333333 |         0.333333 |     0.333333 |\n","output_type":"stream"}],"execution_count":7},{"id":"8733242b","cell_type":"markdown","source":"## Create Dataset\n\nOk I think we now can load the huggingface stuff to create the datasets from the\npandas dataframes?","metadata":{}},{"id":"4b45f04d","cell_type":"code","source":"from datasets import Dataset \n\n# Convert pandas DataFrame to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\n\n# Split the train dataset into train and validation sets, since the test.csv data only has 3 rows.\ntrain_dataset = train_dataset.train_test_split(test_size=0.1, shuffle=True)\n\n# Can see it's now a DatasetDict with 'train' and 'test' splits\ntrain_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:39:31.106967Z","iopub.execute_input":"2025-06-19T21:39:31.107204Z","iopub.status.idle":"2025-06-19T21:39:33.764039Z","shell.execute_reply.started":"2025-06-19T21:39:31.107186Z","shell.execute_reply":"2025-06-19T21:39:33.763296Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n        num_rows: 51729\n    })\n    test: Dataset({\n        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n        num_rows: 5748\n    })\n})"},"metadata":{}}],"execution_count":8},{"id":"87fa7501","cell_type":"markdown","source":"## The model stuff now?\n\nWe need to pick:\n- Model\n- Fine tuning method\n\nLet's start small:\n- smol-lm\n- prompt tuning with `peft`","metadata":{}},{"id":"5737f8e3","cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M\"\ndevice = \"cuda\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\n# why is it none tho\nassert model.config.pad_token_id is None\nassert tokenizer.eos_token is not None, \"Tokenizer must have an eos_token set.\"\n\n# set the pad token to be the same as the eos token\ntokenizer.pad_token = tokenizer.eos_token\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\n\nprint(\"Generated code:\")\nprint(tokenizer.decode(outputs[0]))\nprint(\"---------------\")\n\nprint(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:39:33.764833Z","iopub.execute_input":"2025-06-19T21:39:33.765344Z","iopub.status.idle":"2025-06-19T21:40:09.353486Z","shell.execute_reply.started":"2025-06-19T21:39:33.765325Z","shell.execute_reply":"2025-06-19T21:40:09.352834Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1167bba1f0d49ecada4802d48f2bfb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a68574b91e704a4b878e7f630b78e099"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c85e976493124ab395192e3c44ec4c9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe58dd2ac3234ac2b04fe6ce72aae22b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1e5a83fedea407b965830a66753dc87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da97066183934c9cb774a5b58655f0f7"}},"metadata":{}},{"name":"stderr","text":"2025-06-19 21:39:50.275785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750369190.481071      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750369190.537652      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b728f6eb98d41bf902f8e0b547a7720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69facac24f54441fbeeef4ebe26548e5"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Generated code:\ndef print_hello_world():\n    print(\"Hello World!\")\n\ndef print_hello_world_with_print():\n   \n---------------\nMemory footprint: 538.06 MB\n","output_type":"stream"}],"execution_count":9},{"id":"c3294d80","cell_type":"markdown","source":"## Data format\n\nNote that columns `prompt`, `response_a`, and `response_b` are strings\ncontaining JSON arrays that could have more than 1 element.","metadata":{}},{"id":"df57e8f0","cell_type":"code","source":"import json\n\n# grab the column as a plain Python list of strings\ncol = train_dataset[\"train\"][\"response_a\"]\n\n# find the first row with multiple items\nfirst_multi = next(\n    (\n        (i, arr)\n        for i, raw in enumerate(col)\n        for arr in [json.loads(raw)]\n        if isinstance(arr, list) and len(arr) > 1\n    ),\n    None\n)\n\nif first_multi:\n    i, arr = first_multi\n    print(f\"First row with >1 element: row {i}: {arr}\")\n\n    # now pretty-print the full row at index i\n    row = train_dataset[\"train\"][i].copy()\n\n    # parse the JSON-encoded fields\n    row[\"prompt\"]     = json.loads(row[\"prompt\"])\n    row[\"response_a\"] = arr\n    row[\"response_b\"] = json.loads(row[\"response_b\"])\n\n    print(\"\\nRow detail:\")\n    print(json.dumps(row, indent=2))\nelse:\n    print(\"No rows with >1 element found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:40:09.355663Z","iopub.execute_input":"2025-06-19T21:40:09.356180Z","iopub.status.idle":"2025-06-19T21:40:09.645753Z","shell.execute_reply.started":"2025-06-19T21:40:09.356155Z","shell.execute_reply":"2025-06-19T21:40:09.644909Z"}},"outputs":[{"name":"stdout","text":"First row with >1 element: row 0: ['As a peasant living in the village, I would likely be very frightened and panicked if an army of orcs were to attack. My first thought would be to protect myself and my loved ones, so I would try to gather them together and flee the village as quickly as possible. I would likely be thinking about where we can go to safety and how we can get there.\\n\\nMy plans would likely involve trying to reach the nearest town or city, where we might be able to find shelter and safety. I would try to avoid any areas where the orcs might be, and would likely take a route that I know well so that we can move quickly and avoid getting lost.\\n\\nIn the short term, my main goal would be to get as far away from the orcs as possible and to find a place where we can be safe. In the longer term, I would hope to find a way to defeat the orcs and drive them away, so that we can return to our village and our normal lives.', 'If I chose to reach the nearby castle, my plans would likely involve following the road or a path that leads in that direction. I would try to move as quickly as possible, while also being cautious and staying alert for any signs of danger.\\n\\nAlong the way, I would be thinking about what we might need to bring with us, such as food, water, and other supplies. I would also be trying to remember any information that might be useful, such as the location of any other safe places that we could go if we needed to.\\n\\nWhen we reached the castle, my hope would be that it would be a strong and secure place where we could find refuge and safety. I would likely be looking for the nearest entrance and trying to find a way inside, while also being aware of any potential dangers that might be waiting for us.\\n\\nOnce we were inside the castle, my plans would be to try to find a place where we could hide and rest for a while. I would be looking for a room or area that was secure and where we could be safe from any further danger. I would also be thinking about what we might need to do to stay alive and survive, such as finding food and water and staying warm.']\n\nRow detail:\n{\n  \"id\": 1483993167,\n  \"model_a\": \"vicuna-13b\",\n  \"model_b\": \"koala-13b\",\n  \"prompt\": [\n    \"You are a peasant living in the village. But suddenly army of orcs attack and you have to flee. What are your thoughts? What are your plans? What are you going to do?\",\n    \"You choose to reach nearby castle.\"\n  ],\n  \"response_a\": [\n    \"As a peasant living in the village, I would likely be very frightened and panicked if an army of orcs were to attack. My first thought would be to protect myself and my loved ones, so I would try to gather them together and flee the village as quickly as possible. I would likely be thinking about where we can go to safety and how we can get there.\\n\\nMy plans would likely involve trying to reach the nearest town or city, where we might be able to find shelter and safety. I would try to avoid any areas where the orcs might be, and would likely take a route that I know well so that we can move quickly and avoid getting lost.\\n\\nIn the short term, my main goal would be to get as far away from the orcs as possible and to find a place where we can be safe. In the longer term, I would hope to find a way to defeat the orcs and drive them away, so that we can return to our village and our normal lives.\",\n    \"If I chose to reach the nearby castle, my plans would likely involve following the road or a path that leads in that direction. I would try to move as quickly as possible, while also being cautious and staying alert for any signs of danger.\\n\\nAlong the way, I would be thinking about what we might need to bring with us, such as food, water, and other supplies. I would also be trying to remember any information that might be useful, such as the location of any other safe places that we could go if we needed to.\\n\\nWhen we reached the castle, my hope would be that it would be a strong and secure place where we could find refuge and safety. I would likely be looking for the nearest entrance and trying to find a way inside, while also being aware of any potential dangers that might be waiting for us.\\n\\nOnce we were inside the castle, my plans would be to try to find a place where we could hide and rest for a while. I would be looking for a room or area that was secure and where we could be safe from any further danger. I would also be thinking about what we might need to do to stay alive and survive, such as finding food and water and staying warm.\"\n  ],\n  \"response_b\": [\n    \"As a peasant living in a village, my thoughts when faced with an attack by orcs would likely be focused on my own safety and the safety of my family and loved ones. My plans would likely be to try to find a safe place to hide or escape, such as a nearby fortified settlement or a cave. I would likely also try to gather information about the orcs and their movements, in order to better plan my escape and stay safe.\",\n    \"As a peasant, my thoughts when choosing to reach a nearby castle would likely be focused on the safety and security it would provide. I would likely consider the strength of the castle's walls and gates, as well as the presence of any armed guards or soldiers within the fortress. I would also consider the distance and the amount of time it would take to reach the castle, and whether I would have the necessary supplies and resources to make it safely.\"\n  ],\n  \"winner_model_a\": 1,\n  \"winner_model_b\": 0,\n  \"winner_tie\": 0\n}\n","output_type":"stream"}],"execution_count":10},{"id":"e1bf73e4","cell_type":"markdown","source":"## Preprocessing the Data\n\nNow i want to format the input training data to be an input to the model.\n\nNote that there can be multi-turn conversations.\nThis will be a text input with the following format:\n\n```text\n## Turn 1\n### Prompt\n<prompt[0]>\n\n### Response A\n<response_a[0]>\n\n### Response B\n<response_b[0]>\n\n## Turn 2\n### Prompt\n<prompt[1]>\n\n### Response A\n<response_a[1]>\n\n### Response B\n<response_b[1]>\n\n---\n\nWhich is better?\nAnswer:\n```\n\nWhere `<label>` is one of `a`, `b`, or `tie`.","metadata":{}},{"id":"5535dd3b","cell_type":"code","source":"def preprocess_function(\n    examples,\n    tokenizer,\n    max_length: int = 1024,  # Increased from 512 for multi-turn conversations\n):\n    \"\"\"\n    More efficient preprocessing with better label alignment and proper padding.\n    \"\"\"\n    # 1) Build the text inputs in the desired format\n    inputs = []\n    for prompt_json, response_a_json, response_b_json in zip(\n        examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"]\n    ):\n        # JSON decode the columns to handle multi-turn conversations\n        prompts = json.loads(prompt_json)\n        responses_a = json.loads(response_a_json)\n        responses_b = json.loads(response_b_json)\n        \n        # Build conversation with turn-by-turn format\n        conversation_parts = []\n        for i, (prompt_turn, response_a_turn, response_b_turn) in enumerate(zip(prompts, responses_a, responses_b), 1):\n            turn_text = f\"## Turn {i}\\n\"\n            turn_text += \"### Prompt\\n\"\n            turn_text += f\"{prompt_turn}\\n\\n\"\n\n            turn_text += \"### Response A\\n\"\n            turn_text += f\"{response_a_turn}\\n\\n\"\n\n            turn_text += \"### Response B\\n\"\n            turn_text += f\"{response_b_turn}\\n\"\n\n            conversation_parts.append(turn_text)\n        \n        # Join all turns with separator and add final question\n        conversation = \"\\n---\\n\\n\".join(conversation_parts)\n        input_text = f\"{conversation}\\n\\nWhich is better?\\nAnswer: \"  # Added space after colon\n        inputs.append(input_text)\n\n    # 2) Build the target responses\n    targets = []\n    for wa, wb, wt in zip(\n        examples[\"winner_model_a\"],\n        examples[\"winner_model_b\"],\n        examples[\"winner_tie\"],\n    ):\n        if wa == 1:\n            targets.append(\"a\")\n        elif wb == 1:\n            targets.append(\"b\")\n        elif wt == 1:\n            targets.append(\"tie\")\n        else:\n            raise ValueError(\"Invalid winner values: must be one of a, b, or tie.\")\n\n    # 3) More efficient tokenization - separate input and target tokenization\n    # Tokenize inputs first with no padding to get raw lengths\n    input_tokens = tokenizer(inputs, add_special_tokens=True, padding=False, truncation=False)\n    target_tokens = tokenizer(targets, add_special_tokens=False, padding=False, truncation=False)\n    \n    # 4) Combine and create labels more reliably with proper padding\n    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    \n    for i, (inp_ids, tgt_ids) in enumerate(zip(input_tokens[\"input_ids\"], target_tokens[\"input_ids\"])):\n        # Combine input + target\n        combined_ids = inp_ids + tgt_ids\n        \n        # Truncate if needed - prioritize keeping the full target\n        if len(combined_ids) > max_length:\n            target_len = len(tgt_ids)\n            if target_len < max_length:  # Only truncate if we can fit the target\n                input_truncated = combined_ids[:max_length - target_len]\n                combined_ids = input_truncated + tgt_ids\n            else:\n                # If target itself is too long, truncate everything\n                combined_ids = combined_ids[:max_length]\n        \n        # Create labels: -100 for input part, actual tokens for target part\n        input_len = len(inp_ids) if len(combined_ids) > len(inp_ids) else len(combined_ids) - len(tgt_ids)\n        input_len = max(0, input_len)  # Ensure non-negative\n        \n        labels = [-100] * input_len + combined_ids[input_len:]\n        \n        # Ensure labels match combined_ids length\n        if len(labels) != len(combined_ids):\n            labels = labels[:len(combined_ids)]\n        \n        # Pad sequences to max_length for consistent batching\n        # Pad input_ids and attention_mask\n        attention_mask = [1] * len(combined_ids)\n        \n        if len(combined_ids) < max_length:\n            padding_length = max_length - len(combined_ids)\n            combined_ids += [tokenizer.pad_token_id] * padding_length\n            attention_mask += [0] * padding_length\n            labels += [-100] * padding_length\n        \n        model_inputs[\"input_ids\"].append(combined_ids)\n        model_inputs[\"labels\"].append(labels)\n        model_inputs[\"attention_mask\"].append(attention_mask)\n    \n    return model_inputs\n\n# Test preprocessing on the first row as an example\nexample = train_dataset[\"train\"].select(range(1))\nexample_preprocessed = preprocess_function(\n    example,\n    tokenizer=tokenizer,\n)\n\nprint(\"Improved Preprocessed example:\")\nprint(\"Input length:\", len(example_preprocessed[\"input_ids\"][0]))\nprint(\"Labels length:\", len(example_preprocessed[\"labels\"][0]))\nprint(\"Lengths match:\", len(example_preprocessed[\"input_ids\"][0]) == len(example_preprocessed[\"labels\"][0]))\n\nprint(\"\\nInput IDs:\")\nprint(\"---------------------\")\nprint(tokenizer.decode(example_preprocessed[\"input_ids\"][0]))\n\n# Remove -100 from labels for display\nlabels = [token_id for token_id in example_preprocessed[\"labels\"][0] if token_id != -100]\nprint(\"---------------------\")\nprint(\"Decoded Labels:\")\nprint(tokenizer.decode(labels))\n\n# Show where labels start\nlabels_full = example_preprocessed[\"labels\"][0]\nfirst_non_ignore = next((i for i, x in enumerate(labels_full) if x != -100), None)\nprint(f\"\\nFirst non-ignore label at position: {first_non_ignore}\")\nif first_non_ignore and first_non_ignore > 5:\n    context_start = max(0, first_non_ignore - 5)\n    context_end = min(len(example_preprocessed[\"input_ids\"][0]), first_non_ignore + 5)\n    print(f\"Context around label start: {tokenizer.decode(example_preprocessed['input_ids'][0][context_start:context_end])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:40:09.646573Z","iopub.execute_input":"2025-06-19T21:40:09.646812Z","iopub.status.idle":"2025-06-19T21:40:09.671127Z","shell.execute_reply.started":"2025-06-19T21:40:09.646794Z","shell.execute_reply":"2025-06-19T21:40:09.670407Z"}},"outputs":[{"name":"stdout","text":"Improved Preprocessed example:\nInput length: 1024\nLabels length: 1024\nLengths match: True\n\nInput IDs:\n---------------------\n## Turn 1\n### Prompt\nYou are a peasant living in the village. But suddenly army of orcs attack and you have to flee. What are your thoughts? What are your plans? What are you going to do?\n\n### Response A\nAs a peasant living in the village, I would likely be very frightened and panicked if an army of orcs were to attack. My first thought would be to protect myself and my loved ones, so I would try to gather them together and flee the village as quickly as possible. I would likely be thinking about where we can go to safety and how we can get there.\n\nMy plans would likely involve trying to reach the nearest town or city, where we might be able to find shelter and safety. I would try to avoid any areas where the orcs might be, and would likely take a route that I know well so that we can move quickly and avoid getting lost.\n\nIn the short term, my main goal would be to get as far away from the orcs as possible and to find a place where we can be safe. In the longer term, I would hope to find a way to defeat the orcs and drive them away, so that we can return to our village and our normal lives.\n\n### Response B\nAs a peasant living in a village, my thoughts when faced with an attack by orcs would likely be focused on my own safety and the safety of my family and loved ones. My plans would likely be to try to find a safe place to hide or escape, such as a nearby fortified settlement or a cave. I would likely also try to gather information about the orcs and their movements, in order to better plan my escape and stay safe.\n\n---\n\n## Turn 2\n### Prompt\nYou choose to reach nearby castle.\n\n### Response A\nIf I chose to reach the nearby castle, my plans would likely involve following the road or a path that leads in that direction. I would try to move as quickly as possible, while also being cautious and staying alert for any signs of danger.\n\nAlong the way, I would be thinking about what we might need to bring with us, such as food, water, and other supplies. I would also be trying to remember any information that might be useful, such as the location of any other safe places that we could go if we needed to.\n\nWhen we reached the castle, my hope would be that it would be a strong and secure place where we could find refuge and safety. I would likely be looking for the nearest entrance and trying to find a way inside, while also being aware of any potential dangers that might be waiting for us.\n\nOnce we were inside the castle, my plans would be to try to find a place where we could hide and rest for a while. I would be looking for a room or area that was secure and where we could be safe from any further danger. I would also be thinking about what we might need to do to stay alive and survive, such as finding food and water and staying warm.\n\n### Response B\nAs a peasant, my thoughts when choosing to reach a nearby castle would likely be focused on the safety and security it would provide. I would likely consider the strength of the castle's walls and gates, as well as the presence of any armed guards or soldiers within the fortress. I would also consider the distance and the amount of time it would take to reach the castle, and whether I would have the necessary supplies and resources to make it safely.\n\n\nWhich is better?\nAnswer: a<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n---------------------\nDecoded Labels:\na\n\nFirst non-ignore label at position: 739\nContext around label start: ?\nAnswer: a<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n","output_type":"stream"}],"execution_count":11},{"id":"e9572fbb","cell_type":"code","source":"tokenized = train_dataset.map(\n    lambda ex: preprocess_function(ex, tokenizer),\n    batched=True,\n    # optionally drop old columns\n    remove_columns=train_dataset[\"train\"].column_names,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:40:09.671880Z","iopub.execute_input":"2025-06-19T21:40:09.672094Z","iopub.status.idle":"2025-06-19T21:41:29.230053Z","shell.execute_reply.started":"2025-06-19T21:40:09.672071Z","shell.execute_reply":"2025-06-19T21:41:29.229448Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/51729 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aafe3e50fee4c2fab6173e62614a311"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (8411 > 8192). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8577380ddf746de81cecc43b53f28e9"}},"metadata":{}}],"execution_count":12},{"id":"4ec08a3e","cell_type":"code","source":"# Let's check the first 2 rows of the tokenized dataset\n\nprint(\"Example rows from the tokenized dataset:\")\nprint(\"--- input_ids ---\")\nprint(tokenizer.decode(tokenized[\"train\"][0][\"input_ids\"], skip_special_tokens=True))\nprint(\"---- labels -----\")\n\n# Clear the -100 padding from labels for display\nlabels = [token_id for token_id in tokenized[\"train\"][0][\"labels\"] if token_id != -100]\nprint(tokenizer.decode(labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:41:29.231026Z","iopub.execute_input":"2025-06-19T21:41:29.231277Z","iopub.status.idle":"2025-06-19T21:41:29.241736Z","shell.execute_reply.started":"2025-06-19T21:41:29.231259Z","shell.execute_reply":"2025-06-19T21:41:29.240982Z"}},"outputs":[{"name":"stdout","text":"Example rows from the tokenized dataset:\n--- input_ids ---\n## Turn 1\n### Prompt\nYou are a peasant living in the village. But suddenly army of orcs attack and you have to flee. What are your thoughts? What are your plans? What are you going to do?\n\n### Response A\nAs a peasant living in the village, I would likely be very frightened and panicked if an army of orcs were to attack. My first thought would be to protect myself and my loved ones, so I would try to gather them together and flee the village as quickly as possible. I would likely be thinking about where we can go to safety and how we can get there.\n\nMy plans would likely involve trying to reach the nearest town or city, where we might be able to find shelter and safety. I would try to avoid any areas where the orcs might be, and would likely take a route that I know well so that we can move quickly and avoid getting lost.\n\nIn the short term, my main goal would be to get as far away from the orcs as possible and to find a place where we can be safe. In the longer term, I would hope to find a way to defeat the orcs and drive them away, so that we can return to our village and our normal lives.\n\n### Response B\nAs a peasant living in a village, my thoughts when faced with an attack by orcs would likely be focused on my own safety and the safety of my family and loved ones. My plans would likely be to try to find a safe place to hide or escape, such as a nearby fortified settlement or a cave. I would likely also try to gather information about the orcs and their movements, in order to better plan my escape and stay safe.\n\n---\n\n## Turn 2\n### Prompt\nYou choose to reach nearby castle.\n\n### Response A\nIf I chose to reach the nearby castle, my plans would likely involve following the road or a path that leads in that direction. I would try to move as quickly as possible, while also being cautious and staying alert for any signs of danger.\n\nAlong the way, I would be thinking about what we might need to bring with us, such as food, water, and other supplies. I would also be trying to remember any information that might be useful, such as the location of any other safe places that we could go if we needed to.\n\nWhen we reached the castle, my hope would be that it would be a strong and secure place where we could find refuge and safety. I would likely be looking for the nearest entrance and trying to find a way inside, while also being aware of any potential dangers that might be waiting for us.\n\nOnce we were inside the castle, my plans would be to try to find a place where we could hide and rest for a while. I would be looking for a room or area that was secure and where we could be safe from any further danger. I would also be thinking about what we might need to do to stay alive and survive, such as finding food and water and staying warm.\n\n### Response B\nAs a peasant, my thoughts when choosing to reach a nearby castle would likely be focused on the safety and security it would provide. I would likely consider the strength of the castle's walls and gates, as well as the presence of any armed guards or soldiers within the fortress. I would also consider the distance and the amount of time it would take to reach the castle, and whether I would have the necessary supplies and resources to make it safely.\n\n\nWhich is better?\nAnswer: a\n---- labels -----\na\n","output_type":"stream"}],"execution_count":13},{"id":"4e0f660d","cell_type":"markdown","source":"## Now what\n\nNow we have a dataset in the right format with both the inputs and the labels,\nwe can now train wowowow\n\nLet's use the `peft` library to try prompt tuning","metadata":{}},{"id":"5fa2725d","cell_type":"code","source":"from transformers import default_data_collator\n\n# Use the tokenized datasets directly with the Trainer\ntrain_ds = tokenized[\"train\"]\neval_ds = tokenized[\"test\"]\n\nprint(f\"Training dataset size: {len(train_ds)}\")\nprint(f\"Evaluation dataset size: {len(eval_ds)}\")\n\n# Check that all sequences are now the same length\nprint(f\"\\nChecking sequence lengths consistency:\")\nfirst_sample_length = len(train_ds[0][\"input_ids\"])\nprint(f\"First sample length: {first_sample_length}\")\n\n# Check a few more samples to ensure consistency\nfor i in range(min(5, len(train_ds))):\n    length = len(train_ds[i][\"input_ids\"])\n    labels_length = len(train_ds[i][\"labels\"])\n    attention_length = len(train_ds[i][\"attention_mask\"])\n    print(f\"Sample {i}: input_ids={length}, labels={labels_length}, attention_mask={attention_length}\")\n    \n    if length != labels_length or length != attention_length:\n        print(f\"WARNING: Length mismatch in sample {i}\")\n        break\nelse:\n    print(\"All samples have consistent lengths!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:41:29.242491Z","iopub.execute_input":"2025-06-19T21:41:29.242710Z","iopub.status.idle":"2025-06-19T21:41:29.326647Z","shell.execute_reply.started":"2025-06-19T21:41:29.242695Z","shell.execute_reply":"2025-06-19T21:41:29.326092Z"}},"outputs":[{"name":"stdout","text":"Training dataset size: 51729\nEvaluation dataset size: 5748\n\nChecking sequence lengths consistency:\nFirst sample length: 1024\nSample 0: input_ids=1024, labels=1024, attention_mask=1024\nSample 1: input_ids=1024, labels=1024, attention_mask=1024\nSample 2: input_ids=1024, labels=1024, attention_mask=1024\nSample 3: input_ids=1024, labels=1024, attention_mask=1024\nSample 4: input_ids=1024, labels=1024, attention_mask=1024\nAll samples have consistent lengths!\n","output_type":"stream"}],"execution_count":14},{"id":"fc1f576d","cell_type":"markdown","source":"## PEFT Config\n\nWe use p-tuning instead of prefix tuning or prompt tuning.\n\n### Why?\nPrefix Tuning is more suitable for generation, while we're doing classification\n\nPrompt tuning is very parameter efficient (e.g. could only need to train 8-16 embeddings) but can underperform.","metadata":{}},{"id":"5e1da1dd","cell_type":"code","source":"from peft import PromptEncoderConfig, get_peft_model\n\n# Improved PEFT configuration with more capacity and regularization\npeft_config = PromptEncoderConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=50,\n    encoder_hidden_size=256,\n    encoder_dropout=0.1,\n)\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:41:29.327268Z","iopub.execute_input":"2025-06-19T21:41:29.327485Z","iopub.status.idle":"2025-06-19T21:41:29.665504Z","shell.execute_reply.started":"2025-06-19T21:41:29.327460Z","shell.execute_reply":"2025-06-19T21:41:29.664770Z"}},"outputs":[{"name":"stdout","text":"trainable params: 390,336 || all params: 134,905,344 || trainable%: 0.2893\n","output_type":"stream"}],"execution_count":15},{"id":"c2b3e2e8","cell_type":"markdown","source":"## Training\n\nSetup optimizer and learning rate scheduler.","metadata":{}},{"id":"34913291","cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\nimport torch\nfrom typing import Dict, List, Any\n\n# Since we're already padding in preprocessing, use a simpler data collator\n# that doesn't try to pad again\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  # We're doing causal LM, not masked LM\n    pad_to_multiple_of=None,  # Don't pad again since we already padded in preprocessing\n    return_tensors=\"pt\"\n)\n\n# Improved training arguments with better optimization and scheduling\ntraining_args = TrainingArguments(\n    output_dir=\"./llm-classification-ft-peft-p-tuning/output\",\n    learning_rate=3e-4,              # More conservative learning rate for fine-tuning\n\n    # Smaller batch sizes for memory efficiency\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=4,    # Keep eval batch size higher\n\n    # Increase gradient accumulation to maintain effective batch size\n    gradient_accumulation_steps=16,  # Effective batch size = 2 * 16 = 32\n    # num_train_epochs=0.01,              # More epochs for better convergence\n    max_steps=200, # TEMP: For testing without training too long\n    weight_decay=0.01,\n    \n    # Better optimization settings\n    warmup_ratio=0.1,                # Warmup for training stability\n    lr_scheduler_type=\"cosine\",      # Cosine annealing instead of linear\n    \n    # Better evaluation and saving strategy\n    eval_strategy=\"steps\",           # Evaluate more frequently\n    eval_steps=100,                  # Evaluate every 100 steps\n    save_strategy=\"steps\", \n    save_steps=100,                  # Save every 100 steps\n    save_total_limit=2,              # Limit checkpoints to save disk space\n\n    # Early stopping and best model selection\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n\n    # Logging and reporting\n    report_to=\"wandb\" if not ON_KAGGLE else None,\n    run_name=\"llm-classification-ft-peft-p-tuning-improved\",\n    logging_steps=10,                # More frequent logging\n\n    # Memory optimization settings\n    dataloader_pin_memory=False,     # Disable pin memory to save GPU memory\n    dataloader_num_workers=0,        # Avoid multiprocessing overhead\n    gradient_checkpointing=True,     # Trade compute for memory\n    fp16=True,                       # Use half precision to reduce memory usage\n\n    # Additional optimizations\n    remove_unused_columns=False,     # Important for custom preprocessing\n    label_names=[\"labels\"],          # Fix for PEFT model warning\n    torch_empty_cache_steps=50,      # Clear cache periodically\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:41:29.666286Z","iopub.execute_input":"2025-06-19T21:41:29.666540Z","iopub.status.idle":"2025-06-19T21:41:31.651036Z","shell.execute_reply.started":"2025-06-19T21:41:29.666523Z","shell.execute_reply":"2025-06-19T21:41:31.650470Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/63541181.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":16},{"id":"dea0d3cd","cell_type":"markdown","source":"## Now actually train!","metadata":{}},{"id":"7495ac46","cell_type":"code","source":"# Print current GPU memory usage\nimport torch\n\ndef print_gpu_memory_usage():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        \n        print(\"GPU Memory Usage:\")\n        print(f\"  Allocated: {allocated:.2f} GB\")\n        print(f\"  Reserved:  {reserved:.2f} GB\") \n        print(f\"  Total:     {total:.2f} GB\")\n        print(f\"  Free:      {total - reserved:.2f} GB\")\n        print(f\"  Usage:     {allocated/total*100:.1f}%\")\n    else:\n        print(\"CUDA is not available.\")\n\nprint(\"BEFORE training:\")\nprint_gpu_memory_usage()\n\n# Clear any cached memory\ntorch.cuda.empty_cache()\nprint(\"\\nAfter clearing cache:\")\nprint_gpu_memory_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:41:31.651672Z","iopub.execute_input":"2025-06-19T21:41:31.651866Z","iopub.status.idle":"2025-06-19T21:41:31.658274Z","shell.execute_reply.started":"2025-06-19T21:41:31.651850Z","shell.execute_reply":"2025-06-19T21:41:31.657299Z"}},"outputs":[{"name":"stdout","text":"BEFORE training:\nGPU Memory Usage:\n  Allocated: 0.52 GB\n  Reserved:  0.53 GB\n  Total:     15.89 GB\n  Free:      15.36 GB\n  Usage:     3.3%\n\nAfter clearing cache:\nGPU Memory Usage:\n  Allocated: 0.52 GB\n  Reserved:  0.53 GB\n  Total:     15.89 GB\n  Free:      15.36 GB\n  Usage:     3.3%\n","output_type":"stream"}],"execution_count":17},{"id":"5fe06e37","cell_type":"code","source":"# Memory optimization before training\nimport gc\nimport os\n\n# Clear Python garbage collector\ngc.collect()\n\n# Clear CUDA cache\ntorch.cuda.empty_cache()\n\n# Set PYTORCH environment variable for memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nprint(\"Pre-training memory optimization complete\")\nprint_gpu_memory_usage()\n\n# Check model's memory footprint\nprint(f\"\\nModel memory footprint: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n\n# Print trainable parameters info\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:41:31.659245Z","iopub.execute_input":"2025-06-19T21:41:31.659589Z","iopub.status.idle":"2025-06-19T21:41:32.119836Z","shell.execute_reply.started":"2025-06-19T21:41:31.659542Z","shell.execute_reply":"2025-06-19T21:41:32.119035Z"}},"outputs":[{"name":"stdout","text":"Pre-training memory optimization complete\nGPU Memory Usage:\n  Allocated: 0.52 GB\n  Reserved:  0.53 GB\n  Total:     15.89 GB\n  Free:      15.36 GB\n  Usage:     3.3%\n\nModel memory footprint: 0.50 GB\ntrainable params: 390,336 || all params: 134,905,344 || trainable%: 0.2893\n","output_type":"stream"}],"execution_count":18},{"id":"87698fa4","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:41:32.120712Z","iopub.execute_input":"2025-06-19T21:41:32.121485Z","iopub.status.idle":"2025-06-19T22:39:57.373603Z","shell.execute_reply.started":"2025-06-19T21:41:32.121459Z","shell.execute_reply":"2025-06-19T22:39:57.373005Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250619_214132-ly5wxv11</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft/runs/ly5wxv11' target=\"_blank\">llm-classification-ft-peft-p-tuning-improved</a></strong> to <a href='https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft' target=\"_blank\">https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft/runs/ly5wxv11' target=\"_blank\">https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft/runs/ly5wxv11</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 57:57, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.677900</td>\n      <td>1.704264</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.631200</td>\n      <td>1.686188</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./llm-classification-ft-peft-p-tuning/output/checkpoint-100)... Done. 0.0s\n\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./llm-classification-ft-peft-p-tuning/output/checkpoint-200)... Done. 0.0s\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=1.7521914196014405, metrics={'train_runtime': 3497.5596, 'train_samples_per_second': 1.83, 'train_steps_per_second': 0.057, 'total_flos': 4176089815449600.0, 'train_loss': 1.7521914196014405, 'epoch': 0.12371931181132804})"},"metadata":{}}],"execution_count":19},{"id":"021ef7e1","cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T22:39:57.374311Z","iopub.execute_input":"2025-06-19T22:39:57.374513Z","iopub.status.idle":"2025-06-19T22:40:10.177544Z","shell.execute_reply.started":"2025-06-19T22:39:57.374498Z","shell.execute_reply":"2025-06-19T22:40:10.176812Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▂▇█▂▄▃▄▇▃▅▅▄▅▃▃▁▂▂▁▁</td></tr><tr><td>train/learning_rate</td><td>▄████▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▄▆▄▄▄▅▂▄▁▂▂▃▂▃▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.68619</td></tr><tr><td>eval/runtime</td><td>454.4569</td></tr><tr><td>eval/samples_per_second</td><td>12.648</td></tr><tr><td>eval/steps_per_second</td><td>3.162</td></tr><tr><td>total_flos</td><td>4176089815449600.0</td></tr><tr><td>train/epoch</td><td>0.12372</td></tr><tr><td>train/global_step</td><td>200</td></tr><tr><td>train/grad_norm</td><td>0.54011</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6312</td></tr><tr><td>train_loss</td><td>1.75219</td></tr><tr><td>train_runtime</td><td>3497.5596</td></tr><tr><td>train_samples_per_second</td><td>1.83</td></tr><tr><td>train_steps_per_second</td><td>0.057</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">llm-classification-ft-peft-p-tuning-improved</strong> at: <a href='https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft/runs/ly5wxv11' target=\"_blank\">https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft/runs/ly5wxv11</a><br> View project at: <a href='https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft' target=\"_blank\">https://wandb.ai/drklee3-kava-labs/llm-classification-ft-peft</a><br>Synced 5 W&B file(s), 0 media file(s), 24 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250619_214132-ly5wxv11/logs</code>"},"metadata":{}}],"execution_count":20},{"id":"355ca9cc","cell_type":"code","source":"# Save the model and tokenizer\ntrainer.save_model(\"./llm-classification-ft-peft-p-tuning-improved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T22:40:10.178360Z","iopub.execute_input":"2025-06-19T22:40:10.178638Z","iopub.status.idle":"2025-06-19T22:40:10.628747Z","shell.execute_reply.started":"2025-06-19T22:40:10.178617Z","shell.execute_reply":"2025-06-19T22:40:10.627974Z"}},"outputs":[],"execution_count":21},{"id":"3f4e73e5","cell_type":"markdown","source":"## Inference Time\n\nNow we have a fine tuned model, we can use it to make predictions on the test\nset to see how well (or more likely how poorly) it does.\n\nThis is done in the next notebook that uses this one as an input!","metadata":{}}]}