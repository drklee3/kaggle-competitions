{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"18793371","cell_type":"markdown","source":"# LLM Classification Finetuning\n\nCompetition: https://www.kaggle.com/competitions/llm-classification-finetuning/overview\n\n## Submission File\n\nFor each ID in the test set, you must predict the probability for each target class. The file should contain a header and have the following format:\n\n```csv\nid,winner_model_a,winner_model_b,winner_tie\n136060,0.33,0,33,0.33\n211333,0.33,0,33,0.33\n1233961,0.33,0,33,0.33\netc\n```\n\nSubmission file must be named `submission.csv` in the `/kaggle/working/` directory.\n\n## Inputs\n\nInput files are in `/kaggle/input/llm-classification-finetuning/` directory if\nrunning on Kaggle.\n\n```\n/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n```","metadata":{}},{"id":"76c343d7","cell_type":"code","source":"import os\nkaggle_run_type = os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\nprint(f\"KAGGLE_KERNEL_RUN_TYPE: {kaggle_run_type}\")\n\nON_KAGGLE = kaggle_run_type is not None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T23:19:00.907947Z","iopub.execute_input":"2025-06-23T23:19:00.908458Z","iopub.status.idle":"2025-06-23T23:19:00.912749Z","shell.execute_reply.started":"2025-06-23T23:19:00.908437Z","shell.execute_reply":"2025-06-23T23:19:00.911900Z"}},"outputs":[{"name":"stdout","text":"KAGGLE_KERNEL_RUN_TYPE: Interactive\n","output_type":"stream"}],"execution_count":13},{"id":"115dc832","cell_type":"code","source":"# Download required packages - for re-use in inference notebook\nif ON_KAGGLE:\n    %pip download torch torchvision pandas tabulate transformers evaluate peft wandb \\\n        --dest /kaggle/working/frozen_packages \\\n        --prefer-binary \\\n    \n    # Install required packages\n    %pip install torch torchvision pandas tabulate transformers evaluate peft wandb \\\n        --find-links /kaggle/working/frozen_packages \\\n        --no-index\n\nelse:\n    %pip install torch torchvision pandas tabulate transformers evaluate peft wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T23:19:00.914684Z","iopub.execute_input":"2025-06-23T23:19:00.914914Z"}},"outputs":[{"name":"stdout","text":"Collecting torch\n  File was already downloaded /kaggle/working/frozen_packages/torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl\nCollecting torchvision\n  File was already downloaded /kaggle/working/frozen_packages/torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl\nCollecting pandas\n  File was already downloaded /kaggle/working/frozen_packages/pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting tabulate\n  File was already downloaded /kaggle/working/frozen_packages/tabulate-0.9.0-py3-none-any.whl\nCollecting transformers\n  File was already downloaded /kaggle/working/frozen_packages/transformers-4.52.4-py3-none-any.whl\nCollecting evaluate\n  File was already downloaded /kaggle/working/frozen_packages/evaluate-0.4.4-py3-none-any.whl\nCollecting peft\n  File was already downloaded /kaggle/working/frozen_packages/peft-0.15.2-py3-none-any.whl\nCollecting wandb\n  File was already downloaded /kaggle/working/frozen_packages/wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting filelock (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/filelock-3.18.0-py3-none-any.whl\nCollecting typing-extensions>=4.10.0 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/typing_extensions-4.14.0-py3-none-any.whl\nCollecting sympy>=1.13.3 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/sympy-1.14.0-py3-none-any.whl\nCollecting networkx (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/networkx-3.5-py3-none-any.whl\nCollecting jinja2 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/jinja2-3.1.6-py3-none-any.whl\nCollecting fsspec (from torch)\n  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl\nCollecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl\nCollecting nvidia-nccl-cu12==2.26.2 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nCollecting triton==3.3.1 (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nCollecting setuptools>=40.8.0 (from triton==3.3.1->torch)\n  File was already downloaded /kaggle/working/frozen_packages/setuptools-80.9.0-py3-none-any.whl\nCollecting numpy (from torchvision)\n  File was already downloaded /kaggle/working/frozen_packages/numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl\nCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n  File was already downloaded /kaggle/working/frozen_packages/pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl\nCollecting python-dateutil>=2.8.2 (from pandas)\n  File was already downloaded /kaggle/working/frozen_packages/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\nCollecting pytz>=2020.1 (from pandas)\n  File was already downloaded /kaggle/working/frozen_packages/pytz-2025.2-py2.py3-none-any.whl\nCollecting tzdata>=2022.7 (from pandas)\n  File was already downloaded /kaggle/working/frozen_packages/tzdata-2025.2-py2.py3-none-any.whl\nCollecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n  File was already downloaded /kaggle/working/frozen_packages/huggingface_hub-0.33.0-py3-none-any.whl\nCollecting packaging>=20.0 (from transformers)\n  File was already downloaded /kaggle/working/frozen_packages/packaging-25.0-py3-none-any.whl\nCollecting pyyaml>=5.1 (from transformers)\n  File was already downloaded /kaggle/working/frozen_packages/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting regex!=2019.12.17 (from transformers)\n  File was already downloaded /kaggle/working/frozen_packages/regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting requests (from transformers)\n  File was already downloaded /kaggle/working/frozen_packages/requests-2.32.4-py3-none-any.whl\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  File was already downloaded /kaggle/working/frozen_packages/tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting safetensors>=0.4.3 (from transformers)\n  File was already downloaded /kaggle/working/frozen_packages/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting tqdm>=4.27 (from transformers)\n  File was already downloaded /kaggle/working/frozen_packages/tqdm-4.67.1-py3-none-any.whl\nCollecting datasets>=2.0.0 (from evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/datasets-3.6.0-py3-none-any.whl\nCollecting dill (from evaluate)\n  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\nCollecting xxhash (from evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting multiprocess (from evaluate)\n  Using cached multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\nCollecting psutil (from peft)\n  File was already downloaded /kaggle/working/frozen_packages/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting accelerate>=0.21.0 (from peft)\n  File was already downloaded /kaggle/working/frozen_packages/accelerate-1.8.1-py3-none-any.whl\nCollecting click!=8.0.0,>=7.1 (from wandb)\n  File was already downloaded /kaggle/working/frozen_packages/click-8.2.1-py3-none-any.whl\nCollecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n  File was already downloaded /kaggle/working/frozen_packages/GitPython-3.1.44-py3-none-any.whl\nCollecting platformdirs (from wandb)\n  File was already downloaded /kaggle/working/frozen_packages/platformdirs-4.3.8-py3-none-any.whl\nCollecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n  File was already downloaded /kaggle/working/frozen_packages/protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl\nCollecting pydantic<3 (from wandb)\n  File was already downloaded /kaggle/working/frozen_packages/pydantic-2.11.7-py3-none-any.whl\nCollecting sentry-sdk>=2.0.0 (from wandb)\n  File was already downloaded /kaggle/working/frozen_packages/sentry_sdk-2.30.0-py2.py3-none-any.whl\nCollecting setproctitle (from wandb)\n  File was already downloaded /kaggle/working/frozen_packages/setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl\nCollecting dill (from evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/dill-0.3.8-py3-none-any.whl\nCollecting multiprocess (from evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/multiprocess-0.70.16-py311-none-any.whl\nCollecting fsspec (from torch)\n  File was already downloaded /kaggle/working/frozen_packages/fsspec-2025.3.0-py3-none-any.whl\nCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2021.05.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/aiohttp-3.12.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n  File was already downloaded /kaggle/working/frozen_packages/gitdb-4.0.12-py3-none-any.whl\nCollecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n  File was already downloaded /kaggle/working/frozen_packages/hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n  File was already downloaded /kaggle/working/frozen_packages/annotated_types-0.7.0-py3-none-any.whl\nCollecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n  File was already downloaded /kaggle/working/frozen_packages/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n  File was already downloaded /kaggle/working/frozen_packages/typing_inspection-0.4.1-py3-none-any.whl\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n  File was already downloaded /kaggle/working/frozen_packages/six-1.17.0-py2.py3-none-any.whl\nCollecting charset_normalizer<4,>=2 (from requests->transformers)\n  File was already downloaded /kaggle/working/frozen_packages/charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting idna<4,>=2.5 (from requests->transformers)\n  File was already downloaded /kaggle/working/frozen_packages/idna-3.10-py3-none-any.whl\nCollecting urllib3<3,>=1.21.1 (from requests->transformers)\n  File was already downloaded /kaggle/working/frozen_packages/urllib3-2.5.0-py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests->transformers)\n  File was already downloaded /kaggle/working/frozen_packages/certifi-2025.6.15-py3-none-any.whl\nCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n  File was already downloaded /kaggle/working/frozen_packages/mpmath-1.3.0-py3-none-any.whl\nCollecting MarkupSafe>=2.0 (from jinja2->torch)\n  File was already downloaded /kaggle/working/frozen_packages/MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/aiohappyeyeballs-2.6.1-py3-none-any.whl\nCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/aiosignal-1.3.2-py2.py3-none-any.whl\nCollecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/attrs-25.3.0-py3-none-any.whl\nCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/multidict-6.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\n  File was already downloaded /kaggle/working/frozen_packages/yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n  File was already downloaded /kaggle/working/frozen_packages/smmap-5.0.2-py3-none-any.whl\nSuccessfully downloaded torch nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-cufile-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-cusparselt-cu12 nvidia-nccl-cu12 nvidia-nvjitlink-cu12 nvidia-nvtx-cu12 triton torchvision pandas tabulate transformers evaluate peft wandb accelerate click datasets dill fsspec gitpython huggingface-hub multiprocess numpy packaging pillow protobuf psutil pydantic pydantic-core python-dateutil pytz pyyaml regex requests safetensors sentry-sdk sympy tokenizers tqdm typing-extensions tzdata filelock jinja2 networkx platformdirs setproctitle xxhash aiohttp annotated-types certifi charset_normalizer gitdb hf-xet idna MarkupSafe mpmath pyarrow setuptools six typing-inspection urllib3 aiohappyeyeballs aiosignal attrs frozenlist multidict propcache smmap yarl\nNote: you may need to restart the kernel to use updated packages.\nLooking in links: /kaggle/working/frozen_packages\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.1)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":null},{"id":"012b4c69","cell_type":"code","source":"# Setup wandb\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\nos.environ[\"WANDB_PROJECT\"] = \"llm-classification-ft-peft\"\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n\nif ON_KAGGLE:\n    os.environ[\"WANDB_HOST\"] = \"kaggle\"\n    \n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n    wandb.login(key=wandb_key)\nelse:\n    wandb.login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f0893984","cell_type":"code","source":"BASE_PATH = '/kaggle/input/llm-classification-finetuning' if ON_KAGGLE else './data/'\n\nprint(f\"Using base path: {BASE_PATH}\")\n\nprint(\"Available files in base path:\")\nfor root, dirs, files in os.walk(BASE_PATH):\n    for file in files:\n        print(f\" - {os.path.join(root, file)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5acea90a","cell_type":"markdown","source":"# Data Inputs\n\nLet's load and look at what we got first for inputs.","metadata":{}},{"id":"e3410a4a","cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))\n\nsample_submission_df = pd.read_csv(os.path.join(BASE_PATH, 'sample_submission.csv'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b4e59919","cell_type":"code","source":"print(f\"Train DataFrame shape: {train_df.shape}\")\nprint(f\"Test DataFrame shape: {test_df.shape}\")\nprint(f\"Sample Submission DataFrame shape: {sample_submission_df.shape}\")\n\nprint (\"-------------------------\")\n# Print types of each column\nprint(\"\\nColumn types in Train DataFrame:\")\nprint(train_df.dtypes)\n\nprint(\"\\nColumn types in Test DataFrame:\")\nprint(test_df.dtypes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7c30020a","cell_type":"code","source":"print(\"First rows of each DataFrame:\")\n\nprint(\"\\nTrain DataFrame:\")\nprint(train_df.head(1).to_markdown())\n\nprint(\"\\nTest DataFrame:\")\nprint(test_df.head(1).to_markdown())\n\nprint(\"\\nSample Submission DataFrame:\")\nprint(sample_submission_df.head(1).to_markdown())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8733242b","cell_type":"markdown","source":"## Create Dataset\n\nOk I think we now can load the huggingface stuff to create the datasets from the\npandas dataframes?","metadata":{}},{"id":"4b45f04d","cell_type":"code","source":"from datasets import Dataset \n\n# Convert pandas DataFrame to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\n\n# Split the train dataset into train and validation sets, since the test.csv data only has 3 rows.\ntrain_dataset = train_dataset.train_test_split(test_size=0.1, shuffle=True)\n\n# Can see it's now a DatasetDict with 'train' and 'test' splits\ntrain_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"87fa7501","cell_type":"markdown","source":"## The model stuff now?\n\nWe need to pick:\n- Model\n- Fine tuning method\n\nLet's start small:\n- smol-lm\n- prompt tuning with `peft`","metadata":{}},{"id":"5737f8e3","cell_type":"code","source":"from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n\ncheckpoint = \"answerdotai/ModernBERT-large\"\ndevice = \"cuda\"\n\ncheckpoint = \"answerdotai/ModernBERT-large\"\nconfig     = AutoConfig.from_pretrained(checkpoint)\nconfig._attn_implementation = \"torch\"    # ← no FlashAttention\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config).to(device)\n\n# why is it none tho\nassert model.config.pad_token_id is None\nassert tokenizer.eos_token is not None, \"Tokenizer must have an eos_token set.\"\n\ntext = \"The capital of France is [MASK].\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# To get predictions for the mask:\nmasked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\npredicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\npredicted_token = tokenizer.decode(predicted_token_id)\nprint(\"Predicted token:\", predicted_token)\n\nprint(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c3294d80","cell_type":"markdown","source":"## Data format\n\nNote that columns `prompt`, `response_a`, and `response_b` are strings\ncontaining JSON arrays that could have more than 1 element.","metadata":{}},{"id":"df57e8f0","cell_type":"code","source":"import json\n\n# grab the column as a plain Python list of strings\ncol = train_dataset[\"train\"][\"response_a\"]\n\n# find the first row with multiple items\nfirst_multi = next(\n    (\n        (i, arr)\n        for i, raw in enumerate(col)\n        for arr in [json.loads(raw)]\n        if isinstance(arr, list) and len(arr) > 1\n    ),\n    None\n)\n\nif first_multi:\n    i, arr = first_multi\n    print(f\"First row with >1 element: row {i}: {arr}\")\n\n    # now pretty-print the full row at index i\n    row = train_dataset[\"train\"][i].copy()\n\n    # parse the JSON-encoded fields\n    row[\"prompt\"]     = json.loads(row[\"prompt\"])\n    row[\"response_a\"] = arr\n    row[\"response_b\"] = json.loads(row[\"response_b\"])\n\n    print(\"\\nRow detail:\")\n    print(json.dumps(row, indent=2))\nelse:\n    print(\"No rows with >1 element found.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e1bf73e4","cell_type":"markdown","source":"## Preprocessing the Data\n\nNow i want to format the input training data to be an input to the model.\n\nNote that there can be multi-turn conversations.\nThis will be a text input with the following format:\n\n```text\n[CLS]\nUser: <prompt turn 1>\nAssistant: <response A turn 1>\nUser: <prompt turn 2>\nAssistant: <response A turn 2>\n…\nUser: <prompt turn N>\nAssistant: <response A turn N>\n[SEP]\nUser: <prompt turn 1>\nAssistant: <response B turn 1>\nUser: <prompt turn 2>\nAssistant: <response B turn 2>\n…\nUser: <prompt turn N>\nAssistant: <response B turn N>\n[SEP]\n```\n","metadata":{}},{"id":"5535dd3b","cell_type":"code","source":"def preprocess_function(\n    examples: Dict[str, List[str]],\n    tokenizer: PreTrainedTokenizer,\n    max_length: int = 1024,\n) -> Dict[str, Any]:\n    \"\"\"\n    Preprocess examples for pairwise BERT classification: builds two sequences (history+resp_a, history+resp_b),\n    tokenizes them with special tokens ([CLS], [SEP]), and returns input IDs, attention masks, token type IDs, and labels.\n    \"\"\"\n    seq_as, seq_bs, labels = [], [], []\n    for prompt_json, resp_a_json, resp_b_json, wa, wb, wt in zip(\n        examples[\"prompt\"],\n        examples[\"response_a\"],\n        examples[\"response_b\"],\n        examples[\"winner_model_a\"],\n        examples[\"winner_model_b\"],\n        examples[\"winner_tie\"],\n    ):\n        prompts = json.loads(prompt_json)\n        responses_a = json.loads(resp_a_json)\n        responses_b = json.loads(resp_b_json)\n\n        # Build full multi-turn conversation for each variant\n        conv_a = \"\"\n        conv_b = \"\"\n        for prompt_turn, ra, rb in zip(prompts, responses_a, responses_b):\n            conv_a += f\"User: {prompt_turn}\\nAssistant: {ra}\\n\"\n            conv_b += f\"User: {prompt_turn}\\nAssistant: {rb}\\n\"\n\n        seq_as.append(conv_a.strip())\n        seq_bs.append(conv_b.strip())\n\n        # Map winners to labels: 0=A, 1=B, 2=tie\n        if wa == 1:\n            labels.append(0)\n        elif wb == 1:\n            labels.append(1)\n        elif wt == 1:\n            labels.append(2)\n        else:\n            raise ValueError(f\"Invalid winner flags: {wa}, {wb}, {wt}\")\n\n    # Tokenize pairs with special tokens\n    tokenized = tokenizer(\n        seq_as,\n        seq_bs,\n        add_special_tokens=True,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\",\n    )\n\n    tokenized[\"labels\"] = labels\n    return tokenized\n\n\n# Test preprocessing on the first row as an example\nexample = train_dataset[\"train\"].select(range(1))\nexample_preprocessed = preprocess_function(\n    example,\n    tokenizer=tokenizer,\n)\n\nprint(\"Improved Preprocessed example:\")\nprint(\"Input length:\", len(example_preprocessed[\"input_ids\"][0]))\nprint(\"Labels length:\", len(example_preprocessed[\"labels\"][0]))\nprint(\"Lengths match:\", len(example_preprocessed[\"input_ids\"][0]) == len(example_preprocessed[\"labels\"][0]))\n\nprint(\"\\nInput IDs:\")\nprint(\"---------------------\")\nprint(tokenizer.decode(example_preprocessed[\"input_ids\"][0]))\n\n# Remove -100 from labels for display\nlabels = [token_id for token_id in example_preprocessed[\"labels\"][0] if token_id != -100]\nprint(\"---------------------\")\nprint(\"Decoded Labels:\")\nprint(tokenizer.decode(labels))\n\n# Show where labels start\nlabels_full = example_preprocessed[\"labels\"][0]\nfirst_non_ignore = next((i for i, x in enumerate(labels_full) if x != -100), None)\nprint(f\"\\nFirst non-ignore label at position: {first_non_ignore}\")\nif first_non_ignore and first_non_ignore > 5:\n    context_start = max(0, first_non_ignore - 5)\n    context_end = min(len(example_preprocessed[\"input_ids\"][0]), first_non_ignore + 5)\n    print(f\"Context around label start: {tokenizer.decode(example_preprocessed['input_ids'][0][context_start:context_end])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e9572fbb","cell_type":"code","source":"tokenized = train_dataset.map(\n    lambda ex: preprocess_function(ex, tokenizer),\n    batched=True,\n    # optionally drop old columns\n    remove_columns=train_dataset[\"train\"].column_names,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4ec08a3e","cell_type":"code","source":"# Let's check the first 2 rows of the tokenized dataset\n\nprint(\"Example rows from the tokenized dataset:\")\nprint(\"--- input_ids ---\")\nprint(tokenizer.decode(tokenized[\"train\"][0][\"input_ids\"], skip_special_tokens=True))\nprint(\"---- labels -----\")\n\n# Clear the -100 padding from labels for display\nlabels = [token_id for token_id in tokenized[\"train\"][0][\"labels\"] if token_id != -100]\nprint(tokenizer.decode(labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4e0f660d","cell_type":"markdown","source":"## Now what\n\nNow we have a dataset in the right format with both the inputs and the labels,\nwe can now train wowowow\n\nLet's use the `peft` library to try prompt tuning","metadata":{}},{"id":"5fa2725d","cell_type":"code","source":"from transformers import default_data_collator\n\n# Use the tokenized datasets directly with the Trainer\ntrain_ds = tokenized[\"train\"]\neval_ds = tokenized[\"test\"]\n\nprint(f\"Training dataset size: {len(train_ds)}\")\nprint(f\"Evaluation dataset size: {len(eval_ds)}\")\n\n# Check that all sequences are now the same length\nprint(f\"\\nChecking sequence lengths consistency:\")\nfirst_sample_length = len(train_ds[0][\"input_ids\"])\nprint(f\"First sample length: {first_sample_length}\")\n\n# Check a few more samples to ensure consistency\nfor i in range(min(5, len(train_ds))):\n    length = len(train_ds[i][\"input_ids\"])\n    labels_length = len(train_ds[i][\"labels\"])\n    attention_length = len(train_ds[i][\"attention_mask\"])\n    print(f\"Sample {i}: input_ids={length}, labels={labels_length}, attention_mask={attention_length}\")\n    \n    if length != labels_length or length != attention_length:\n        print(f\"WARNING: Length mismatch in sample {i}\")\n        break\nelse:\n    print(\"All samples have consistent lengths!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fc1f576d","cell_type":"markdown","source":"## PEFT Config\n\nWe use p-tuning instead of prefix tuning or prompt tuning.\n\n### Why?\nPrefix Tuning is more suitable for generation, while we're doing classification\n\nPrompt tuning is very parameter efficient (e.g. could only need to train 8-16 embeddings) but can underperform.","metadata":{}},{"id":"5e1da1dd","cell_type":"code","source":"from peft import PromptEncoderConfig, get_peft_model\n\n# Improved PEFT configuration with more capacity and regularization\npeft_config = PromptEncoderConfig(\n    task_type=\"CAUSAL_LM\",\n    num_virtual_tokens=50,\n    encoder_hidden_size=256,\n    encoder_dropout=0.1,\n)\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c2b3e2e8","cell_type":"markdown","source":"## Training Setup\n\nSetup optimizer and learning rate scheduler.","metadata":{}},{"id":"34913291","cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\nimport torch\nfrom typing import Dict, List, Any\n\n# Since we're already padding in preprocessing, use a simpler data collator\n# that doesn't try to pad again\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  # We're doing causal LM, not masked LM\n    pad_to_multiple_of=None,  # Don't pad again since we already padded in preprocessing\n    return_tensors=\"pt\"\n)\n\n# Improved training arguments with better optimization and scheduling\ntraining_args = TrainingArguments(\n    output_dir=\"./llm-classification-ft-peft-p-tuning/output\",\n    learning_rate=3e-4,              # More conservative learning rate for fine-tuning\n\n    # Smaller batch sizes for memory efficiency\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=4,    # Keep eval batch size higher\n\n    # Increase gradient accumulation to maintain effective batch size\n    gradient_accumulation_steps=16,  # Effective batch size = 2 * 16 = 32\n    # num_train_epochs=0.01,              # More epochs for better convergence\n    max_steps=10, # TEMP: For testing without training too long\n    weight_decay=0.01,\n    \n    # Better optimization settings\n    warmup_ratio=0.1,                # Warmup for training stability\n    lr_scheduler_type=\"cosine\",      # Cosine annealing instead of linear\n    \n    # Better evaluation and saving strategy\n    eval_strategy=\"steps\",           # Evaluate more frequently\n    eval_steps=100,                  # Evaluate every 100 steps\n    save_strategy=\"steps\", \n    save_steps=100,                  # Save every 100 steps\n    save_total_limit=2,              # Limit checkpoints to save disk space\n\n    # Early stopping and best model selection\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n\n    # Logging and reporting\n    report_to=\"wandb\" if not ON_KAGGLE else None,\n    run_name=\"llm-classification-ft-peft-p-tuning-improved\",\n    logging_steps=10,                # More frequent logging\n\n    # Memory optimization settings\n    dataloader_pin_memory=False,     # Disable pin memory to save GPU memory\n    dataloader_num_workers=0,        # Avoid multiprocessing overhead\n    gradient_checkpointing=True,     # Trade compute for memory\n    fp16=True,                       # Use half precision to reduce memory usage\n\n    # Additional optimizations\n    remove_unused_columns=False,     # Important for custom preprocessing\n    label_names=[\"labels\"],          # Fix for PEFT model warning\n    torch_empty_cache_steps=50,      # Clear cache periodically\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dea0d3cd","cell_type":"markdown","source":"## Now actually train!","metadata":{}},{"id":"7495ac46","cell_type":"code","source":"# Print current GPU memory usage\nimport torch\n\ndef print_gpu_memory_usage():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        \n        print(\"GPU Memory Usage:\")\n        print(f\"  Allocated: {allocated:.2f} GB\")\n        print(f\"  Reserved:  {reserved:.2f} GB\") \n        print(f\"  Total:     {total:.2f} GB\")\n        print(f\"  Free:      {total - reserved:.2f} GB\")\n        print(f\"  Usage:     {allocated/total*100:.1f}%\")\n    else:\n        print(\"CUDA is not available.\")\n\nprint(\"BEFORE training:\")\nprint_gpu_memory_usage()\n\n# Clear any cached memory\ntorch.cuda.empty_cache()\nprint(\"\\nAfter clearing cache:\")\nprint_gpu_memory_usage()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5fe06e37","cell_type":"code","source":"# Memory optimization before training\nimport gc\nimport os\n\n# Clear Python garbage collector\ngc.collect()\n\n# Clear CUDA cache\ntorch.cuda.empty_cache()\n\n# Set PYTORCH environment variable for memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nprint(\"Pre-training memory optimization complete\")\nprint_gpu_memory_usage()\n\n# Check model's memory footprint\nprint(f\"\\nModel memory footprint: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n\n# Print trainable parameters info\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"87698fa4","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"021ef7e1","cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"355ca9cc","cell_type":"code","source":"# Save the model and tokenizer\noutput_path = \"/kaggle/working/model-output\" if ON_KAGGLE else \"./model-ouput\"\ntrainer.save_model(output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"add838c0-0477-49b3-a031-4a71ab1e44f6","cell_type":"code","source":"# Also save the full base model & tokenizer for offline use in the\n# next inference notebook\nbase_output_path = \"/kaggle/working/base-model\" if ON_KAGGLE else \"./base-model\"\n\n# get the underlying HF model out of your PEFT wrapper\nbase_model = model.base_model\n\n# save both model weights/config and tokenizer\nbase_model.save_pretrained(base_output_path)\ntokenizer.save_pretrained(base_output_path)\n\nprint(f\"Base model and tokenizer saved to {base_output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3f4e73e5","cell_type":"markdown","source":"## Inference Time\n\nNow we have a fine tuned model, we can use it to make predictions on the test\nset to see how well (or more likely how poorly) it does.\n\nThis is done in the next notebook that uses this one as an input!","metadata":{}}]}