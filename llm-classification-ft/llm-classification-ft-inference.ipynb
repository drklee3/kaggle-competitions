{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54568331",
   "metadata": {
    "papermill": {
     "duration": 0.003401,
     "end_time": "2025-06-20T02:33:55.182514",
     "exception": false,
     "start_time": "2025-06-20T02:33:55.179113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LLM Classification Finetuning Inference\n",
    "\n",
    "This is the **offline** notebook used for inference on Kaggle. This will load\n",
    "the fine tuned model from the training notebook, along with loading dependencies\n",
    "from the training notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Competition: https://www.kaggle.com/competitions/llm-classification-finetuning/overview\n",
    "\n",
    "## Submission File\n",
    "\n",
    "For each ID in the test set, you must predict the probability for each target class. The file should contain a header and have the following format:\n",
    "\n",
    "```csv\n",
    "id,winner_model_a,winner_model_b,winner_tie\n",
    "136060,0.33,0,33,0.33\n",
    "211333,0.33,0,33,0.33\n",
    "1233961,0.33,0,33,0.33\n",
    "etc\n",
    "```\n",
    "\n",
    "Submission file must be named `submission.csv` in the `/kaggle/working/` directory.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Input files are in `/kaggle/input/llm-classification-finetuning/` directory if\n",
    "running on Kaggle.\n",
    "\n",
    "```\n",
    "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
    "/kaggle/input/llm-classification-finetuning/train.csv\n",
    "/kaggle/input/llm-classification-finetuning/test.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68c2d6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:33:55.188467Z",
     "iopub.status.busy": "2025-06-20T02:33:55.188239Z",
     "iopub.status.idle": "2025-06-20T02:33:55.195191Z",
     "shell.execute_reply": "2025-06-20T02:33:55.194444Z"
    },
    "papermill": {
     "duration": 0.011399,
     "end_time": "2025-06-20T02:33:55.196503",
     "exception": false,
     "start_time": "2025-06-20T02:33:55.185104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE_KERNEL_RUN_TYPE: Batch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "kaggle_run_type = os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\n",
    "print(f\"KAGGLE_KERNEL_RUN_TYPE: {kaggle_run_type}\")\n",
    "\n",
    "ON_KAGGLE = kaggle_run_type is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15549c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:33:55.202025Z",
     "iopub.status.busy": "2025-06-20T02:33:55.201820Z",
     "iopub.status.idle": "2025-06-20T02:36:33.868390Z",
     "shell.execute_reply": "2025-06-20T02:36:33.867279Z"
    },
    "papermill": {
     "duration": 158.670761,
     "end_time": "2025-06-20T02:36:33.869690",
     "exception": false,
     "start_time": "2025-06-20T02:33:55.198929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/evaluate-0.4.3-py3-none-any.whl\r\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\r\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/sympy-1.14.0-py3-none-any.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from torch)\r\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\r\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\r\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\r\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\r\n",
      "Processing /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages/fsspec-2025.3.0-py3-none-any.whl (from torch)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Installing collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, evaluate\r\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\r\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\r\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\r\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.2.0\r\n",
      "    Uninstalling triton-3.2.0:\r\n",
      "      Successfully uninstalled triton-3.2.0\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.13.1\r\n",
      "    Uninstalling sympy-1.13.1:\r\n",
      "      Successfully uninstalled sympy-1.13.1\r\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\r\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.3.2\r\n",
      "    Uninstalling fsspec-2025.3.2:\r\n",
      "      Successfully uninstalled fsspec-2025.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.6.0+cu124\r\n",
      "    Uninstalling torch-2.6.0+cu124:\r\n",
      "      Successfully uninstalled torch-2.6.0+cu124\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.21.0+cu124\r\n",
      "    Uninstalling torchvision-0.21.0+cu124:\r\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\r\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\r\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1 triton-3.3.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "if ON_KAGGLE:\n",
    "    %pip install torch torchvision pandas tabulate transformers evaluate peft wandb \\\n",
    "        --find-links /kaggle/input/k/drklee3/llm-classification-finetuning/frozen_packages \\\n",
    "        --no-index\n",
    "else:\n",
    "    %pip install transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20431be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:36:33.883326Z",
     "iopub.status.busy": "2025-06-20T02:36:33.883113Z",
     "iopub.status.idle": "2025-06-20T02:36:33.890425Z",
     "shell.execute_reply": "2025-06-20T02:36:33.889450Z"
    },
    "papermill": {
     "duration": 0.016081,
     "end_time": "2025-06-20T02:36:33.892010",
     "exception": false,
     "start_time": "2025-06-20T02:36:33.875929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input path: /kaggle/input/llm-classification-finetuning\n",
      "Files in: /kaggle/input/llm-classification-finetuning\n",
      " - /kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      " - /kaggle/input/llm-classification-finetuning/train.csv\n",
      " - /kaggle/input/llm-classification-finetuning/test.csv\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH_INPUT = '/kaggle/input/llm-classification-finetuning' if ON_KAGGLE else './data/'\n",
    "BASE_PATH_TRAIN_NB = '/kaggle/input/k/drklee3/llm-classification-finetuning' if ON_KAGGLE else './'\n",
    "\n",
    "print(f\"Using input path: {BASE_PATH_INPUT}\")\n",
    "\n",
    "def list_files(path):\n",
    "    \"\"\"List all files in the given directory and its subdirectories.\"\"\"\n",
    "    print(f\"Files in: {path}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            print(f\" - {os.path.join(root, file)}\")\n",
    "\n",
    "list_files(BASE_PATH_INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac676b",
   "metadata": {
    "papermill": {
     "duration": 0.006645,
     "end_time": "2025-06-20T02:36:33.908194",
     "exception": false,
     "start_time": "2025-06-20T02:36:33.901549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load model trained in training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b6e8c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:36:33.921973Z",
     "iopub.status.busy": "2025-06-20T02:36:33.921743Z",
     "iopub.status.idle": "2025-06-20T02:37:04.456871Z",
     "shell.execute_reply": "2025-06-20T02:37:04.456225Z"
    },
    "papermill": {
     "duration": 30.543935,
     "end_time": "2025-06-20T02:37:04.458396",
     "exception": false,
     "start_time": "2025-06-20T02:36:33.914461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 02:36:45.614074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750387005.846161      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750387005.917279      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Load the model after training, saved locally.\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device         = \"cuda\"\n",
    "base_model_dir = BASE_PATH_TRAIN_NB + \"/base-model\"\n",
    "peft_dir       = BASE_PATH_TRAIN_NB + \"/model-output\"\n",
    "\n",
    "# 1) tokenizer + base\n",
    "tokenizer  = AutoTokenizer.from_pretrained(base_model_dir, local_files_only=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_dir, local_files_only=True).to(device)\n",
    "\n",
    "# 2) attach the adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    peft_dir, \n",
    "    local_files_only=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e14d60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:37:04.473905Z",
     "iopub.status.busy": "2025-06-20T02:37:04.472765Z",
     "iopub.status.idle": "2025-06-20T02:37:06.561162Z",
     "shell.execute_reply": "2025-06-20T02:37:06.560211Z"
    },
    "papermill": {
     "duration": 2.096996,
     "end_time": "2025-06-20T02:37:06.562684",
     "exception": false,
     "start_time": "2025-06-20T02:37:04.465688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1889: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of France?\n",
      "Response: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of\n"
     ]
    }
   ],
   "source": [
    "# Try an inference example\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3630e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:37:06.576704Z",
     "iopub.status.busy": "2025-06-20T02:37:06.576178Z",
     "iopub.status.idle": "2025-06-20T02:37:07.188355Z",
     "shell.execute_reply": "2025-06-20T02:37:07.187121Z"
    },
    "papermill": {
     "duration": 0.620538,
     "end_time": "2025-06-20T02:37:07.189740",
     "exception": false,
     "start_time": "2025-06-20T02:37:06.569202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (3, 4)\n",
      "Test data columns: ['id', 'prompt', 'response_a', 'response_b']\n",
      "First few rows:\n",
      "        id                                             prompt  \\\n",
      "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
      "1   211333  [\"You are a mediator in a heated political deb...   \n",
      "2  1233961  [\"How to initialize the classification head wh...   \n",
      "\n",
      "                                          response_a  \\\n",
      "0                    [\"You have two oranges today.\"]   \n",
      "1  [\"Thank you for sharing the details of the sit...   \n",
      "2  [\"When you want to initialize the classificati...   \n",
      "\n",
      "                                          response_b  \n",
      "0  [\"You still have three oranges. Eating an oran...  \n",
      "1  [\"Mr Reddy and Ms Blue both have valid points ...  \n",
      "2  [\"To initialize the classification head when p...  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function_for_inference(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    max_length: int = 1024,\n",
    "):\n",
    "    \"\"\"\n",
    "    Modified preprocessing function for inference (no labels needed).\n",
    "    Returns only the formatted input text for prediction.\n",
    "    \"\"\"\n",
    "    # Build the text inputs in the desired format (same as training)\n",
    "    inputs = []\n",
    "    for prompt_json, response_a_json, response_b_json in zip(\n",
    "        examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"]\n",
    "    ):\n",
    "        # JSON decode the columns to handle multi-turn conversations\n",
    "        prompts = json.loads(prompt_json)\n",
    "        responses_a = json.loads(response_a_json)\n",
    "        responses_b = json.loads(response_b_json)\n",
    "        \n",
    "        # Build conversation with turn-by-turn format (same as training)\n",
    "        conversation_parts = []\n",
    "        for i, (prompt_turn, response_a_turn, response_b_turn) in enumerate(zip(prompts, responses_a, responses_b), 1):\n",
    "            turn_text = f\"## Turn {i}\\n\"\n",
    "            turn_text += \"### Prompt\\n\"\n",
    "            turn_text += f\"{prompt_turn}\\n\\n\"\n",
    "\n",
    "            turn_text += \"### Response A\\n\"\n",
    "            turn_text += f\"{response_a_turn}\\n\\n\"\n",
    "\n",
    "            turn_text += \"### Response B\\n\"\n",
    "            turn_text += f\"{response_b_turn}\\n\"\n",
    "\n",
    "            conversation_parts.append(turn_text)\n",
    "        \n",
    "        # Join all turns with separator and add final question\n",
    "        conversation = \"\\n---\\n\\n\".join(conversation_parts)\n",
    "        input_text = f\"{conversation}\\n\\nWhich is better?\\nAnswer: \"\n",
    "        inputs.append(input_text)\n",
    "\n",
    "    # Tokenize inputs only (no targets for inference)\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        add_special_tokens=True, \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    return model_inputs.to(device)\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv(f\"{BASE_PATH_INPUT}/test.csv\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Test data columns: {test_df.columns.tolist()}\")\n",
    "print(f\"First few rows:\\n{test_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e26592e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:37:07.203388Z",
     "iopub.status.busy": "2025-06-20T02:37:07.203169Z",
     "iopub.status.idle": "2025-06-20T02:37:07.218581Z",
     "shell.execute_reply": "2025-06-20T02:37:07.218041Z"
    },
    "papermill": {
     "duration": 0.023424,
     "end_time": "2025-06-20T02:37:07.219787",
     "exception": false,
     "start_time": "2025-06-20T02:37:07.196363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f248e7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:37:07.233606Z",
     "iopub.status.busy": "2025-06-20T02:37:07.233395Z",
     "iopub.status.idle": "2025-06-20T02:37:07.240783Z",
     "shell.execute_reply": "2025-06-20T02:37:07.240220Z"
    },
    "papermill": {
     "duration": 0.015817,
     "end_time": "2025-06-20T02:37:07.241758",
     "exception": false,
     "start_time": "2025-06-20T02:37:07.225941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "def predict_with_model(model, tokenizer, test_dataset, batch_size=1):\n",
    "    \"\"\"\n",
    "    Generate predictions for test examples using the fine-tuned model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Eval mode for inference\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Processing {len(test_dataset)} examples...\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(test_dataset), batch_size):\n",
    "        batch_end = min(i + batch_size, len(test_dataset))\n",
    "        batch = test_dataset.select(range(i, batch_end))\n",
    "        \n",
    "        print(f\"## Processing batch {i//batch_size + 1}/{(len(test_dataset) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        # Preprocess the batch\n",
    "        inputs = preprocess_function_for_inference(batch, tokenizer)\n",
    "        \n",
    "        # Move to device if using GPU\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # We only expect \"a\", \"b\", or \"tie\"\n",
    "                do_sample=False,    # Use greedy decoding for consistent results\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            # Decode the generated tokens (skip the input tokens)\n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            generated_tokens = outputs[:, input_length:]\n",
    "\n",
    "            # Decode each response\n",
    "            for j, generated in enumerate(generated_tokens):\n",
    "                response = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "                print(f\"### Example {i+j}\")\n",
    "                print(f\"Response: {response}\")\n",
    "\n",
    "                # Convert response to probabilities\n",
    "                if response.lower().startswith('a'):\n",
    "                    pred = [1.0, 0.0, 0.0]  # winner_model_a\n",
    "                elif response.lower().startswith('b'):\n",
    "                    pred = [0.0, 1.0, 0.0]  # winner_model_b\n",
    "                elif response.lower().startswith('tie'):\n",
    "                    pred = [0.0, 0.0, 1.0]  # winner_tie\n",
    "                else:\n",
    "                    # Default to uniform distribution if unclear\n",
    "                    pred = [0.33, 0.33, 0.34]\n",
    "                    print(\"[Warning] Unclear response, using uniform distribution\")\n",
    "\n",
    "                predictions.append(pred)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3eabe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:37:07.255024Z",
     "iopub.status.busy": "2025-06-20T02:37:07.254821Z",
     "iopub.status.idle": "2025-06-20T02:37:08.383000Z",
     "shell.execute_reply": "2025-06-20T02:37:08.382196Z"
    },
    "papermill": {
     "duration": 1.136637,
     "end_time": "2025-06-20T02:37:08.384341",
     "exception": false,
     "start_time": "2025-06-20T02:37:07.247704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "Processing 3 examples...\n",
      "## Processing batch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1889: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Example 0\n",
      "Response: 1.\n",
      "\n",
      "### Prompt\n",
      "I have three\n",
      "[Warning] Unclear response, using uniform distribution\n",
      "## Processing batch 2/3\n",
      "### Example 1\n",
      "Response: 1.\n",
      "\n",
      "### Prompt\n",
      "You are a\n",
      "[Warning] Unclear response, using uniform distribution\n",
      "## Processing batch 3/3\n",
      "### Example 2\n",
      "Response: learning libraries like PyTorch or TensorFlow to load the model\n",
      "[Warning] Unclear response, using uniform distribution\n",
      "\n",
      "Generated 3 predictions\n",
      "ID 136060\n",
      "  Prediction: [0.33, 0.33, 0.34]\n",
      "ID 211333\n",
      "  Prediction: [0.33, 0.33, 0.34]\n",
      "ID 1233961\n",
      "  Prediction: [0.33, 0.33, 0.34]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for the test set\n",
    "print(\"Generating predictions...\")\n",
    "predictions = predict_with_model(model, tokenizer, test_dataset)\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions)} predictions\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"ID {test_df.iloc[i]['id']}\")\n",
    "    print(f\"  Prediction: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a208e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T02:37:08.398823Z",
     "iopub.status.busy": "2025-06-20T02:37:08.398540Z",
     "iopub.status.idle": "2025-06-20T02:37:08.417432Z",
     "shell.execute_reply": "2025-06-20T02:37:08.416660Z"
    },
    "papermill": {
     "duration": 0.027269,
     "end_time": "2025-06-20T02:37:08.418676",
     "exception": false,
     "start_time": "2025-06-20T02:37:08.391407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission DataFrame:\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060            0.33            0.33        0.34\n",
      "1   211333            0.33            0.33        0.34\n",
      "2  1233961            0.33            0.33        0.34\n",
      "\n",
      "Probability sums - Min: 1.000, Max: 1.000\n",
      "\n",
      "Submission saved to: /kaggle/working/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Create submission dataframe\n",
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'winner_model_a': [pred[0] for pred in predictions],\n",
    "    'winner_model_b': [pred[1] for pred in predictions], \n",
    "    'winner_tie': [pred[2] for pred in predictions]\n",
    "})\n",
    "\n",
    "print(\"Submission DataFrame:\")\n",
    "print(submission_df)\n",
    "\n",
    "# Verify probabilities sum to 1 (approximately)\n",
    "prob_sums = submission_df[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\n",
    "print(f\"\\nProbability sums - Min: {prob_sums.min():.3f}, Max: {prob_sums.max():.3f}\")\n",
    "\n",
    "# Save submission file\n",
    "output_path = \"/kaggle/working/submission.csv\" if ON_KAGGLE else \"./submission.csv\"\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "sourceId": 246425200,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 201.532527,
   "end_time": "2025-06-20T02:37:11.882905",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-20T02:33:50.350378",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
