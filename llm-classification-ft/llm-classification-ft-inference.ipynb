{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07da61f4",
   "metadata": {},
   "source": [
    "# LLM Classification Finetuning Inference\n",
    "\n",
    "This is the **offline** notebook used for inference on Kaggle. This will load\n",
    "the fine tuned model from the training notebook, along with loading dependencies\n",
    "from the training notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Competition: https://www.kaggle.com/competitions/llm-classification-finetuning/overview\n",
    "\n",
    "## Submission File\n",
    "\n",
    "For each ID in the test set, you must predict the probability for each target class. The file should contain a header and have the following format:\n",
    "\n",
    "```csv\n",
    "id,winner_model_a,winner_model_b,winner_tie\n",
    "136060,0.33,0,33,0.33\n",
    "211333,0.33,0,33,0.33\n",
    "1233961,0.33,0,33,0.33\n",
    "etc\n",
    "```\n",
    "\n",
    "Submission file must be named `submission.csv` in the `/kaggle/working/` directory.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Input files are in `/kaggle/input/llm-classification-finetuning/` directory if\n",
    "running on Kaggle.\n",
    "\n",
    "```\n",
    "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
    "/kaggle/input/llm-classification-finetuning/train.csv\n",
    "/kaggle/input/llm-classification-finetuning/test.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779bcbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: peft in /root/miniconda3/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (1.7.0)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /root/miniconda3/lib/python3.10/site-packages (from peft) (1.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c066891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE_KERNEL_RUN_TYPE: None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "kaggle_run_type = os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\n",
    "print(f\"KAGGLE_KERNEL_RUN_TYPE: {kaggle_run_type}\")\n",
    "\n",
    "ON_KAGGLE = kaggle_run_type is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc040df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input path: ./data/\n",
      "Files in: ./data/\n",
      " - ./data/test.csv\n",
      " - ./data/sample_submission.csv\n",
      " - ./data/train.csv\n",
      "Files in: ./\n",
      " - ./Qwen3_(4B)_GRPO.ipynb\n",
      " - ./.ipynb_checkpoints/Qwen3_(4B)_GRPO-checkpoint.ipynb\n",
      " - ./unsloth_compiled_cache/UnslothAlignPropTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothBCOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothCPOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothDDPOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothDPOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothGKDTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothGRPOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothIterativeSFTTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothKTOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothNashMDTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothOnlineDPOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothORPOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothPPOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothPRMTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothRewardTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothRLOOTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothSFTTrainer.py\n",
      " - ./unsloth_compiled_cache/UnslothXPOTrainer.py\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothAlignPropTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothBCOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothCPOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothDDPOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothDPOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothGKDTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothGRPOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothIterativeSFTTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothKTOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothNashMDTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothOnlineDPOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothORPOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothPPOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothPRMTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothRewardTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothRLOOTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothSFTTrainer.cpython-310.pyc\n",
      " - ./unsloth_compiled_cache/__pycache__/UnslothXPOTrainer.cpython-310.pyc\n",
      " - ./outputs/checkpoint-100/README.md\n",
      " - ./outputs/checkpoint-100/adapter_model.safetensors\n",
      " - ./outputs/checkpoint-100/adapter_config.json\n",
      " - ./outputs/checkpoint-100/chat_template.jinja\n",
      " - ./outputs/checkpoint-100/tokenizer_config.json\n",
      " - ./outputs/checkpoint-100/special_tokens_map.json\n",
      " - ./outputs/checkpoint-100/added_tokens.json\n",
      " - ./outputs/checkpoint-100/vocab.json\n",
      " - ./outputs/checkpoint-100/merges.txt\n",
      " - ./outputs/checkpoint-100/tokenizer.json\n",
      " - ./outputs/checkpoint-100/training_args.bin\n",
      " - ./outputs/checkpoint-100/optimizer.pt\n",
      " - ./outputs/checkpoint-100/scheduler.pt\n",
      " - ./outputs/checkpoint-100/rng_state.pth\n",
      " - ./outputs/checkpoint-100/trainer_state.json\n",
      " - ./grpo_trainer_lora_model/adapter_config.json\n",
      " - ./grpo_saved_lora/README.md\n",
      " - ./grpo_saved_lora/adapter_model.safetensors\n",
      " - ./grpo_saved_lora/adapter_config.json\n",
      " - ./wandb/debug.log\n",
      " - ./wandb/debug-internal.log\n",
      " - ./wandb/run-20250606_185218-h26uy91t/run-h26uy91t.wandb\n",
      " - ./wandb/run-20250606_185218-h26uy91t/logs/debug.log\n",
      " - ./wandb/run-20250606_185218-h26uy91t/logs/debug-core.log\n",
      " - ./wandb/run-20250606_185218-h26uy91t/logs/debug-internal.log\n",
      " - ./wandb/run-20250606_185218-h26uy91t/files/output.log\n",
      " - ./wandb/run-20250606_185218-h26uy91t/files/wandb-metadata.json\n",
      " - ./wandb/run-20250606_185218-h26uy91t/files/requirements.txt\n",
      " - ./wandb/run-20250606_185218-h26uy91t/files/wandb-summary.json\n",
      " - ./wandb/run-20250606_185218-h26uy91t/files/config.yaml\n",
      " - ./wandb/run-20250609_193242-0b3viycv/run-0b3viycv.wandb\n",
      " - ./wandb/run-20250609_193242-0b3viycv/logs/debug.log\n",
      " - ./wandb/run-20250609_193242-0b3viycv/logs/debug-core.log\n",
      " - ./wandb/run-20250609_193242-0b3viycv/logs/debug-internal.log\n",
      " - ./wandb/run-20250609_193242-0b3viycv/files/output.log\n",
      " - ./wandb/run-20250609_193242-0b3viycv/files/wandb-metadata.json\n",
      " - ./wandb/run-20250609_193242-0b3viycv/files/requirements.txt\n",
      " - ./wandb/run-20250609_193242-0b3viycv/files/wandb-summary.json\n",
      " - ./wandb/run-20250609_193242-0b3viycv/files/config.yaml\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/run-nkxwdlp4.wandb\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/logs/debug.log\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/logs/debug-core.log\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/logs/debug-internal.log\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/files/output.log\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/files/wandb-metadata.json\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/files/requirements.txt\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/files/wandb-summary.json\n",
      " - ./wandb/run-20250609_193429-nkxwdlp4/files/config.yaml\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/run-j9e92dy3.wandb\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/logs/debug.log\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/logs/debug-core.log\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/logs/debug-internal.log\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/files/output.log\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/files/wandb-metadata.json\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/files/requirements.txt\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/files/wandb-summary.json\n",
      " - ./wandb/run-20250609_200158-j9e92dy3/files/config.yaml\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/run-7x7ru34s.wandb\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/logs/debug.log\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/logs/debug-core.log\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/logs/debug-internal.log\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/files/output.log\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/files/wandb-metadata.json\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/files/requirements.txt\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/files/wandb-summary.json\n",
      " - ./wandb/run-20250609_231829-7x7ru34s/files/config.yaml\n",
      " - ./wandb/run-20250610_194719-zev96az0/run-zev96az0.wandb\n",
      " - ./wandb/run-20250610_194719-zev96az0/logs/debug.log\n",
      " - ./wandb/run-20250610_194719-zev96az0/logs/debug-core.log\n",
      " - ./wandb/run-20250610_194719-zev96az0/logs/debug-internal.log\n",
      " - ./wandb/run-20250610_194719-zev96az0/files/output.log\n",
      " - ./wandb/run-20250610_194719-zev96az0/files/wandb-metadata.json\n",
      " - ./wandb/run-20250610_194719-zev96az0/files/requirements.txt\n",
      " - ./wandb/run-20250610_194719-zev96az0/files/wandb-summary.json\n",
      " - ./wandb/run-20250610_194719-zev96az0/files/config.yaml\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/run-ggvyzr4w.wandb\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/logs/debug.log\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/logs/debug-core.log\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/logs/debug-internal.log\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/files/output.log\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/files/wandb-metadata.json\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/files/requirements.txt\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/files/wandb-summary.json\n",
      " - ./wandb/run-20250610_200054-ggvyzr4w/files/config.yaml\n",
      " - ./wandb/run-20250610_202627-2sr185pd/run-2sr185pd.wandb\n",
      " - ./wandb/run-20250610_202627-2sr185pd/logs/debug.log\n",
      " - ./wandb/run-20250610_202627-2sr185pd/logs/debug-core.log\n",
      " - ./wandb/run-20250610_202627-2sr185pd/logs/debug-internal.log\n",
      " - ./wandb/run-20250610_202627-2sr185pd/files/output.log\n",
      " - ./wandb/run-20250610_202627-2sr185pd/files/wandb-metadata.json\n",
      " - ./wandb/run-20250610_202627-2sr185pd/files/requirements.txt\n",
      " - ./wandb/run-20250610_202627-2sr185pd/files/wandb-summary.json\n",
      " - ./wandb/run-20250610_202627-2sr185pd/files/config.yaml\n",
      " - ./wandb/run-20250610_211452-k47crbs5/run-k47crbs5.wandb\n",
      " - ./wandb/run-20250610_211452-k47crbs5/logs/debug.log\n",
      " - ./wandb/run-20250610_211452-k47crbs5/logs/debug-core.log\n",
      " - ./wandb/run-20250610_211452-k47crbs5/logs/debug-internal.log\n",
      " - ./wandb/run-20250610_211452-k47crbs5/files/output.log\n",
      " - ./wandb/run-20250610_211452-k47crbs5/files/wandb-metadata.json\n",
      " - ./wandb/run-20250610_211452-k47crbs5/files/requirements.txt\n",
      " - ./wandb/run-20250610_211452-k47crbs5/files/wandb-summary.json\n",
      " - ./wandb/run-20250610_211452-k47crbs5/files/config.yaml\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/run-zc39f2s2.wandb\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/logs/debug.log\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/logs/debug-core.log\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/logs/debug-internal.log\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/files/output.log\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/files/wandb-metadata.json\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/files/requirements.txt\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/files/wandb-summary.json\n",
      " - ./wandb/run-20250619_002055-zc39f2s2/files/config.yaml\n",
      " - ./wandb/run-20250619_002637-6133xuuj/run-6133xuuj.wandb\n",
      " - ./wandb/run-20250619_002637-6133xuuj/logs/debug.log\n",
      " - ./wandb/run-20250619_002637-6133xuuj/logs/debug-core.log\n",
      " - ./wandb/run-20250619_002637-6133xuuj/logs/debug-internal.log\n",
      " - ./wandb/run-20250619_002637-6133xuuj/files/output.log\n",
      " - ./wandb/run-20250619_002637-6133xuuj/files/wandb-metadata.json\n",
      " - ./wandb/run-20250619_002637-6133xuuj/files/requirements.txt\n",
      " - ./wandb/run-20250619_002637-6133xuuj/files/wandb-summary.json\n",
      " - ./wandb/run-20250619_002637-6133xuuj/files/config.yaml\n",
      " - ./wandb/run-20250619_003950-jhup8yx9/run-jhup8yx9.wandb\n",
      " - ./wandb/run-20250619_003950-jhup8yx9/logs/debug.log\n",
      " - ./wandb/run-20250619_003950-jhup8yx9/logs/debug-core.log\n",
      " - ./wandb/run-20250619_003950-jhup8yx9/logs/debug-internal.log\n",
      " - ./wandb/run-20250619_003950-jhup8yx9/files/output.log\n",
      " - ./wandb/run-20250619_003950-jhup8yx9/files/wandb-metadata.json\n",
      " - ./wandb/run-20250619_003950-jhup8yx9/files/requirements.txt\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/run-70ksmmq1.wandb\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/logs/debug.log\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/logs/debug-core.log\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/logs/debug-internal.log\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/files/output.log\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/files/wandb-metadata.json\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/files/requirements.txt\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/files/wandb-summary.json\n",
      " - ./wandb/run-20250619_011244-70ksmmq1/files/config.yaml\n",
      " - ./sft_output/checkpoint-100/config.json\n",
      " - ./sft_output/checkpoint-100/generation_config.json\n",
      " - ./sft_output/checkpoint-100/model.safetensors\n",
      " - ./sft_output/checkpoint-100/chat_template.jinja\n",
      " - ./sft_output/checkpoint-100/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-100/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-100/vocab.json\n",
      " - ./sft_output/checkpoint-100/merges.txt\n",
      " - ./sft_output/checkpoint-100/tokenizer.json\n",
      " - ./sft_output/checkpoint-100/training_args.bin\n",
      " - ./sft_output/checkpoint-100/optimizer.pt\n",
      " - ./sft_output/checkpoint-100/scheduler.pt\n",
      " - ./sft_output/checkpoint-100/rng_state.pth\n",
      " - ./sft_output/checkpoint-100/trainer_state.json\n",
      " - ./sft_output/checkpoint-200/config.json\n",
      " - ./sft_output/checkpoint-200/generation_config.json\n",
      " - ./sft_output/checkpoint-200/model.safetensors\n",
      " - ./sft_output/checkpoint-200/chat_template.jinja\n",
      " - ./sft_output/checkpoint-200/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-200/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-200/vocab.json\n",
      " - ./sft_output/checkpoint-200/merges.txt\n",
      " - ./sft_output/checkpoint-200/tokenizer.json\n",
      " - ./sft_output/checkpoint-200/training_args.bin\n",
      " - ./sft_output/checkpoint-200/optimizer.pt\n",
      " - ./sft_output/checkpoint-200/scheduler.pt\n",
      " - ./sft_output/checkpoint-200/rng_state.pth\n",
      " - ./sft_output/checkpoint-200/trainer_state.json\n",
      " - ./sft_output/checkpoint-300/config.json\n",
      " - ./sft_output/checkpoint-300/generation_config.json\n",
      " - ./sft_output/checkpoint-300/model.safetensors\n",
      " - ./sft_output/checkpoint-300/chat_template.jinja\n",
      " - ./sft_output/checkpoint-300/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-300/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-300/vocab.json\n",
      " - ./sft_output/checkpoint-300/merges.txt\n",
      " - ./sft_output/checkpoint-300/tokenizer.json\n",
      " - ./sft_output/checkpoint-300/training_args.bin\n",
      " - ./sft_output/checkpoint-300/optimizer.pt\n",
      " - ./sft_output/checkpoint-300/scheduler.pt\n",
      " - ./sft_output/checkpoint-300/rng_state.pth\n",
      " - ./sft_output/checkpoint-300/trainer_state.json\n",
      " - ./sft_output/checkpoint-400/config.json\n",
      " - ./sft_output/checkpoint-400/generation_config.json\n",
      " - ./sft_output/checkpoint-400/model.safetensors\n",
      " - ./sft_output/checkpoint-400/chat_template.jinja\n",
      " - ./sft_output/checkpoint-400/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-400/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-400/vocab.json\n",
      " - ./sft_output/checkpoint-400/merges.txt\n",
      " - ./sft_output/checkpoint-400/tokenizer.json\n",
      " - ./sft_output/checkpoint-400/training_args.bin\n",
      " - ./sft_output/checkpoint-400/optimizer.pt\n",
      " - ./sft_output/checkpoint-400/scheduler.pt\n",
      " - ./sft_output/checkpoint-400/rng_state.pth\n",
      " - ./sft_output/checkpoint-400/trainer_state.json\n",
      " - ./sft_output/checkpoint-500/config.json\n",
      " - ./sft_output/checkpoint-500/generation_config.json\n",
      " - ./sft_output/checkpoint-500/model.safetensors\n",
      " - ./sft_output/checkpoint-500/chat_template.jinja\n",
      " - ./sft_output/checkpoint-500/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-500/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-500/vocab.json\n",
      " - ./sft_output/checkpoint-500/merges.txt\n",
      " - ./sft_output/checkpoint-500/tokenizer.json\n",
      " - ./sft_output/checkpoint-500/training_args.bin\n",
      " - ./sft_output/checkpoint-500/optimizer.pt\n",
      " - ./sft_output/checkpoint-500/scheduler.pt\n",
      " - ./sft_output/checkpoint-500/rng_state.pth\n",
      " - ./sft_output/checkpoint-500/trainer_state.json\n",
      " - ./sft_output/checkpoint-600/config.json\n",
      " - ./sft_output/checkpoint-600/generation_config.json\n",
      " - ./sft_output/checkpoint-600/model.safetensors\n",
      " - ./sft_output/checkpoint-600/chat_template.jinja\n",
      " - ./sft_output/checkpoint-600/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-600/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-600/vocab.json\n",
      " - ./sft_output/checkpoint-600/merges.txt\n",
      " - ./sft_output/checkpoint-600/tokenizer.json\n",
      " - ./sft_output/checkpoint-600/training_args.bin\n",
      " - ./sft_output/checkpoint-600/optimizer.pt\n",
      " - ./sft_output/checkpoint-600/scheduler.pt\n",
      " - ./sft_output/checkpoint-600/rng_state.pth\n",
      " - ./sft_output/checkpoint-600/trainer_state.json\n",
      " - ./sft_output/checkpoint-700/config.json\n",
      " - ./sft_output/checkpoint-700/generation_config.json\n",
      " - ./sft_output/checkpoint-700/model.safetensors\n",
      " - ./sft_output/checkpoint-700/chat_template.jinja\n",
      " - ./sft_output/checkpoint-700/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-700/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-700/vocab.json\n",
      " - ./sft_output/checkpoint-700/merges.txt\n",
      " - ./sft_output/checkpoint-700/tokenizer.json\n",
      " - ./sft_output/checkpoint-700/training_args.bin\n",
      " - ./sft_output/checkpoint-700/optimizer.pt\n",
      " - ./sft_output/checkpoint-700/scheduler.pt\n",
      " - ./sft_output/checkpoint-700/rng_state.pth\n",
      " - ./sft_output/checkpoint-700/trainer_state.json\n",
      " - ./sft_output/checkpoint-800/config.json\n",
      " - ./sft_output/checkpoint-800/generation_config.json\n",
      " - ./sft_output/checkpoint-800/model.safetensors\n",
      " - ./sft_output/checkpoint-800/chat_template.jinja\n",
      " - ./sft_output/checkpoint-800/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-800/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-800/vocab.json\n",
      " - ./sft_output/checkpoint-800/merges.txt\n",
      " - ./sft_output/checkpoint-800/tokenizer.json\n",
      " - ./sft_output/checkpoint-800/training_args.bin\n",
      " - ./sft_output/checkpoint-800/optimizer.pt\n",
      " - ./sft_output/checkpoint-800/scheduler.pt\n",
      " - ./sft_output/checkpoint-800/rng_state.pth\n",
      " - ./sft_output/checkpoint-800/trainer_state.json\n",
      " - ./sft_output/checkpoint-900/config.json\n",
      " - ./sft_output/checkpoint-900/generation_config.json\n",
      " - ./sft_output/checkpoint-900/model.safetensors\n",
      " - ./sft_output/checkpoint-900/chat_template.jinja\n",
      " - ./sft_output/checkpoint-900/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-900/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-900/vocab.json\n",
      " - ./sft_output/checkpoint-900/merges.txt\n",
      " - ./sft_output/checkpoint-900/tokenizer.json\n",
      " - ./sft_output/checkpoint-900/training_args.bin\n",
      " - ./sft_output/checkpoint-900/optimizer.pt\n",
      " - ./sft_output/checkpoint-900/scheduler.pt\n",
      " - ./sft_output/checkpoint-900/rng_state.pth\n",
      " - ./sft_output/checkpoint-900/trainer_state.json\n",
      " - ./sft_output/checkpoint-1000/config.json\n",
      " - ./sft_output/checkpoint-1000/generation_config.json\n",
      " - ./sft_output/checkpoint-1000/model.safetensors\n",
      " - ./sft_output/checkpoint-1000/chat_template.jinja\n",
      " - ./sft_output/checkpoint-1000/tokenizer_config.json\n",
      " - ./sft_output/checkpoint-1000/special_tokens_map.json\n",
      " - ./sft_output/checkpoint-1000/vocab.json\n",
      " - ./sft_output/checkpoint-1000/merges.txt\n",
      " - ./sft_output/checkpoint-1000/tokenizer.json\n",
      " - ./sft_output/checkpoint-1000/training_args.bin\n",
      " - ./sft_output/checkpoint-1000/optimizer.pt\n",
      " - ./sft_output/checkpoint-1000/scheduler.pt\n",
      " - ./sft_output/checkpoint-1000/rng_state.pth\n",
      " - ./sft_output/checkpoint-1000/trainer_state.json\n",
      " - ./SmolLM2-FT-MyDataset/config.json\n",
      " - ./SmolLM2-FT-MyDataset/generation_config.json\n",
      " - ./SmolLM2-FT-MyDataset/model.safetensors\n",
      " - ./SmolLM2-FT-MyDataset/chat_template.jinja\n",
      " - ./SmolLM2-FT-MyDataset/tokenizer_config.json\n",
      " - ./SmolLM2-FT-MyDataset/special_tokens_map.json\n",
      " - ./SmolLM2-FT-MyDataset/vocab.json\n",
      " - ./SmolLM2-FT-MyDataset/merges.txt\n",
      " - ./SmolLM2-FT-MyDataset/tokenizer.json\n",
      " - ./SmolLM2-FT-MyDataset/training_args.bin\n",
      " - ./SmolLM2-FT-WithCheckpointing/config.json\n",
      " - ./SmolLM2-FT-WithCheckpointing/generation_config.json\n",
      " - ./SmolLM2-FT-WithCheckpointing/model.safetensors\n",
      " - ./SmolLM2-FT-WithCheckpointing/chat_template.jinja\n",
      " - ./SmolLM2-FT-WithCheckpointing/tokenizer_config.json\n",
      " - ./SmolLM2-FT-WithCheckpointing/special_tokens_map.json\n",
      " - ./SmolLM2-FT-WithCheckpointing/vocab.json\n",
      " - ./SmolLM2-FT-WithCheckpointing/merges.txt\n",
      " - ./SmolLM2-FT-WithCheckpointing/tokenizer.json\n",
      " - ./SmolLM2-FT-WithCheckpointing/training_args.bin\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/config.json\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/generation_config.json\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/model.safetensors\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/chat_template.jinja\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/tokenizer_config.json\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/special_tokens_map.json\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/vocab.json\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/merges.txt\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/tokenizer.json\n",
      " - ./models/SmolLM2-FT-WithCheckpointing/training_args.bin\n",
      " - ./SmolLM2-FT-DPO/config.json\n",
      " - ./SmolLM2-FT-DPO/generation_config.json\n",
      " - ./SmolLM2-FT-DPO/model.safetensors\n",
      " - ./SmolLM2-FT-DPO/chat_template.jinja\n",
      " - ./SmolLM2-FT-DPO/tokenizer_config.json\n",
      " - ./SmolLM2-FT-DPO/special_tokens_map.json\n",
      " - ./SmolLM2-FT-DPO/vocab.json\n",
      " - ./SmolLM2-FT-DPO/merges.txt\n",
      " - ./SmolLM2-FT-DPO/tokenizer.json\n",
      " - ./SmolLM2-FT-DPO/training_args.bin\n",
      " - ./data/test.csv\n",
      " - ./data/sample_submission.csv\n",
      " - ./data/train.csv\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/README.md\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/adapter_model.safetensors\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/adapter_config.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/tokenizer_config.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/special_tokens_map.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/vocab.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/merges.txt\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/tokenizer.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/training_args.bin\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/optimizer.pt\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/scheduler.pt\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/scaler.pt\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/rng_state.pth\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4851/trainer_state.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/README.md\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/adapter_model.safetensors\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/adapter_config.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/tokenizer_config.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/special_tokens_map.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/vocab.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/merges.txt\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/tokenizer.json\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/training_args.bin\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/optimizer.pt\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/scheduler.pt\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/scaler.pt\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/rng_state.pth\n",
      " - ./llm-classification-ft-peft-p-tuning/output/checkpoint-4800/trainer_state.json\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/README.md\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/adapter_model.safetensors\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/adapter_config.json\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/tokenizer_config.json\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/special_tokens_map.json\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/vocab.json\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/merges.txt\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/tokenizer.json\n",
      " - ./llm-classification-ft-peft-p-tuning-improved/training_args.bin\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH_INPUT = '/kaggle/input/llm-classification-finetuning' if ON_KAGGLE else './data/'\n",
    "BASE_PATH_TRAIN_NB = '/kaggle/input/k/drklee3/llm-classification-finetuning' if ON_KAGGLE else './'\n",
    "\n",
    "print(f\"Using input path: {BASE_PATH_INPUT}\")\n",
    "\n",
    "def list_files(path):\n",
    "    \"\"\"List all files in the given directory and its subdirectories.\"\"\"\n",
    "    print(f\"Files in: {path}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            print(f\" - {os.path.join(root, file)}\")\n",
    "\n",
    "list_files(BASE_PATH_INPUT)\n",
    "list_files(BASE_PATH_TRAIN_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149a050",
   "metadata": {},
   "source": [
    "## Load model trained in training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model after training\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = BASE_PATH_TRAIN_NB + \"/model-output\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb441077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of France?\n",
      "Response: What is the capital of France?\n",
      "\n",
      "The capital of France is the capital of France.\n",
      "\n",
      "The capital of France is the capital of France.\n",
      "\n",
      "The capital of France is the capital of France.\n",
      "\n",
      "The capital of France is the capital of France.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try an inference example\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddf004b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (3, 4)\n",
      "Test data columns: ['id', 'prompt', 'response_a', 'response_b']\n",
      "First few rows:\n",
      "        id                                             prompt  \\\n",
      "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
      "1   211333  [\"You are a mediator in a heated political deb...   \n",
      "2  1233961  [\"How to initialize the classification head wh...   \n",
      "\n",
      "                                          response_a  \\\n",
      "0                    [\"You have two oranges today.\"]   \n",
      "1  [\"Thank you for sharing the details of the sit...   \n",
      "2  [\"When you want to initialize the classificati...   \n",
      "\n",
      "                                          response_b  \n",
      "0  [\"You still have three oranges. Eating an oran...  \n",
      "1  [\"Mr Reddy and Ms Blue both have valid points ...  \n",
      "2  [\"To initialize the classification head when p...  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function_for_inference(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    max_length: int = 1024,\n",
    "):\n",
    "    \"\"\"\n",
    "    Modified preprocessing function for inference (no labels needed).\n",
    "    Returns only the formatted input text for prediction.\n",
    "    \"\"\"\n",
    "    # Build the text inputs in the desired format (same as training)\n",
    "    inputs = []\n",
    "    for prompt_json, response_a_json, response_b_json in zip(\n",
    "        examples[\"prompt\"], examples[\"response_a\"], examples[\"response_b\"]\n",
    "    ):\n",
    "        # JSON decode the columns to handle multi-turn conversations\n",
    "        prompts = json.loads(prompt_json)\n",
    "        responses_a = json.loads(response_a_json)\n",
    "        responses_b = json.loads(response_b_json)\n",
    "        \n",
    "        # Build conversation with turn-by-turn format (same as training)\n",
    "        conversation_parts = []\n",
    "        for i, (prompt_turn, response_a_turn, response_b_turn) in enumerate(zip(prompts, responses_a, responses_b), 1):\n",
    "            turn_text = f\"## Turn {i}\\n\"\n",
    "            turn_text += \"### Prompt\\n\"\n",
    "            turn_text += f\"{prompt_turn}\\n\\n\"\n",
    "\n",
    "            turn_text += \"### Response A\\n\"\n",
    "            turn_text += f\"{response_a_turn}\\n\\n\"\n",
    "\n",
    "            turn_text += \"### Response B\\n\"\n",
    "            turn_text += f\"{response_b_turn}\\n\"\n",
    "\n",
    "            conversation_parts.append(turn_text)\n",
    "        \n",
    "        # Join all turns with separator and add final question\n",
    "        conversation = \"\\n---\\n\\n\".join(conversation_parts)\n",
    "        input_text = f\"{conversation}\\n\\nWhich is better?\\nAnswer: \"\n",
    "        inputs.append(input_text)\n",
    "\n",
    "    # Tokenize inputs only (no targets for inference)\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        add_special_tokens=True, \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    return model_inputs.to(device)\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv(f\"{BASE_PATH_INPUT}/test.csv\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Test data columns: {test_df.columns.tolist()}\")\n",
    "print(f\"First few rows:\\n{test_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eca28899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1eb55a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "def predict_with_model(model, tokenizer, test_dataset, batch_size=1):\n",
    "    \"\"\"\n",
    "    Generate predictions for test examples using the fine-tuned model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Eval mode for inference\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Processing {len(test_dataset)} examples...\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(test_dataset), batch_size):\n",
    "        batch_end = min(i + batch_size, len(test_dataset))\n",
    "        batch = test_dataset.select(range(i, batch_end))\n",
    "        \n",
    "        print(f\"## Processing batch {i//batch_size + 1}/{(len(test_dataset) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        # Preprocess the batch\n",
    "        inputs = preprocess_function_for_inference(batch, tokenizer)\n",
    "        \n",
    "        # Move to device if using GPU\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # We only expect \"a\", \"b\", or \"tie\"\n",
    "                do_sample=False,    # Use greedy decoding for consistent results\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            # Decode the generated tokens (skip the input tokens)\n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            generated_tokens = outputs[:, input_length:]\n",
    "\n",
    "            # Decode each response\n",
    "            for j, generated in enumerate(generated_tokens):\n",
    "                response = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "                print(f\"### Example {i+j}\")\n",
    "                print(f\"Response: {response}\")\n",
    "\n",
    "                # Convert response to probabilities\n",
    "                if response.lower().startswith('a'):\n",
    "                    pred = [1.0, 0.0, 0.0]  # winner_model_a\n",
    "                elif response.lower().startswith('b'):\n",
    "                    pred = [0.0, 1.0, 0.0]  # winner_model_b\n",
    "                elif response.lower().startswith('tie'):\n",
    "                    pred = [0.0, 0.0, 1.0]  # winner_tie\n",
    "                else:\n",
    "                    # Default to uniform distribution if unclear\n",
    "                    pred = [0.33, 0.33, 0.34]\n",
    "                    print(\"[Warning] Unclear response, using uniform distribution\")\n",
    "\n",
    "                predictions.append(pred)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd8398e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "Processing 3 examples...\n",
      "## Processing batch 1/3\n",
      "### Example 0\n",
      "Response: 1\n",
      "\n",
      "### Prompt\n",
      "I have three oranges\n",
      "[Warning] Unclear response, using uniform distribution\n",
      "## Processing batch 2/3\n",
      "### Example 1\n",
      "Response: 1\n",
      "\n",
      "### Prompt\n",
      "You are a mediator\n",
      "[Warning] Unclear response, using uniform distribution\n",
      "## Processing batch 3/3\n",
      "### Example 2\n",
      "Response: learning libraries like PyTorch or TensorFlow to load the model\n",
      "[Warning] Unclear response, using uniform distribution\n",
      "\n",
      "Generated 3 predictions\n",
      "ID 136060\n",
      "  Prediction: [0.33, 0.33, 0.34]\n",
      "ID 211333\n",
      "  Prediction: [0.33, 0.33, 0.34]\n",
      "ID 1233961\n",
      "  Prediction: [0.33, 0.33, 0.34]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for the test set\n",
    "print(\"Generating predictions...\")\n",
    "predictions = predict_with_model(model, tokenizer, test_dataset)\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions)} predictions\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"ID {test_df.iloc[i]['id']}\")\n",
    "    print(f\"  Prediction: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524a357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission DataFrame:\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060            0.33            0.33        0.34\n",
      "1   211333            0.33            0.33        0.34\n",
      "2  1233961            0.33            0.33        0.34\n",
      "\n",
      "Probability sums - Min: 1.000, Max: 1.000\n",
      "\n",
      "Submission saved to: ./submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Create submission dataframe\n",
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'winner_model_a': [pred[0] for pred in predictions],\n",
    "    'winner_model_b': [pred[1] for pred in predictions], \n",
    "    'winner_tie': [pred[2] for pred in predictions]\n",
    "})\n",
    "\n",
    "print(\"Submission DataFrame:\")\n",
    "print(submission_df)\n",
    "\n",
    "# Verify probabilities sum to 1 (approximately)\n",
    "prob_sums = submission_df[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\n",
    "print(f\"\\nProbability sums - Min: {prob_sums.min():.3f}, Max: {prob_sums.max():.3f}\")\n",
    "\n",
    "# Save submission file\n",
    "output_path = \"/kaggle/working/submission.csv\" if ON_KAGGLE else \"./submission.csv\"\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
